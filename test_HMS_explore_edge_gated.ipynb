{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import edt\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from skimage.metrics import adapted_rand_error\n",
    "\n",
    "import torch\n",
    "from torch import from_numpy as from_numpy\n",
    "from torchsummary import summary\n",
    "\n",
    "from func.run_pipeline_super_vox import segment_super_vox_3_channel, semantic_segment_crop_and_cat_3_channel_output_edge_gated_model, img_3d_erosion_or_expansion, \\\n",
    "generate_super_vox_by_watershed, get_outlayer_of_a_3d_shape, get_crop_by_pixel_val, Cluster_Super_Vox, assign_boudary_voxels_to_cells_with_watershed, \\\n",
    "delete_too_small_cluster, reassign\n",
    "from func.run_pipeline import segment, assign_boudary_voxels_to_cells, dbscan_of_seg, semantic_segment_crop_and_cat\n",
    "from func.cal_accuracy import IOU_and_Dice_Accuracy, VOI\n",
    "from func.network import VoxResNet, CellSegNet_basic_lite, CellSegNet_basic_edge_gated_IV\n",
    "from func.unet_3d_basic import UNet3D_basic\n",
    "from func.ultis import save_obj, load_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "CellSegNet_basic_edge_gated_IV(\n  (conv1): Conv3d(1, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n  (conv2): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  (bnorm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv3): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n  (resmodule1): ResModule(\n    (batchnorm_module): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv_module): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  )\n  (conv4): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n  (resmodule2): ResModule(\n    (batchnorm_module): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv_module): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  )\n  (conv5): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n  (resmodule3): ResModule(\n    (batchnorm_module): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv_module): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  )\n  (deconv1): ConvTranspose3d(64, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n  (bnorm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (deconv2): ConvTranspose3d(64, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n  (bnorm3): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (deconv3): ConvTranspose3d(64, 32, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n  (bnorm4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv6): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  (conv7): Conv3d(32, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n  (edgegatelayer1): EdgeGatedLayer_II(\n    (conv_edge): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n    (conv_main): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n  )\n  (edgegatelayer2): EdgeGatedLayer_II(\n    (conv_edge): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n    (conv_main): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n  )\n  (edgegatelayer3): EdgeGatedLayer_II(\n    (conv_edge): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n    (conv_main): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n  )\n  (edgegatelayer4): EdgeGatedLayer_II(\n    (conv_edge): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n    (conv_main): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n  )\n  (deconv1_edge): ConvTranspose3d(64, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n  (bnorm2_edge): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (deconv2_edge): ConvTranspose3d(64, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n  (bnorm3_edge): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (deconv3_edge): ConvTranspose3d(64, 32, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n  (bnorm4_edge): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv6_edge): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  (conv7_edge): Conv3d(32, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n  (sigmoid_edge): Sigmoid()\n)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model=UNet3D_basic(in_channels = 1, out_channels = 3)\n",
    "# load_path=''\n",
    "# model=VoxResNet(input_channel=1, n_classes=3, output_func = \"softmax\")\n",
    "# load_path=\"\"\n",
    "model=CellSegNet_basic_edge_gated_IV(input_channel=1, n_classes=3, output_func = \"softmax\")\n",
    "load_path='output/model_HMS_5.pkl'\n",
    "checkpoint = torch.load(load_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "#summary(model, (1, 64, 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HMS_data_dict = load_obj(\"dataset_info/HMS_dataset_info\")\n",
    "HMS_data_dict_test = HMS_data_dict[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seg one img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do not input the whole raw image to the model one time but input raw image crops\n",
    "crop_cube_size=64\n",
    "stride=32\n",
    "\n",
    "# hyperparameter for TASCAN, min touching area of two super pixels if they belong to the same cell\n",
    "min_touching_area=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose a test image and load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases: dict_keys(['135', '120', '65', '90'])\n",
      "for test case 135 : {'raw': 'data/CellSeg_dataset/HMS_processed/raw/135.npy', 'background': 'data/CellSeg_dataset/HMS_processed/segmentation/135/135_background_3d_mask.npy', 'edge': 'data/CellSeg_dataset/HMS_processed/segmentation/135/135_edge_3d_mask.npy', 'edge_foreground': 'data/CellSeg_dataset/HMS_processed/segmentation/135/135_edge_foreground_3d_mask.npy', 'edge_background': 'data/CellSeg_dataset/HMS_processed/segmentation/135/135_edge_background_3d_mask.npy', 'boundary': 'data/CellSeg_dataset/HMS_processed/segmentation/135/135_boundary_3d_mask.npy', 'foreground': 'data/CellSeg_dataset/HMS_processed/segmentation/135/135_foreground_3d_mask.npy', 'ins': 'data/CellSeg_dataset/HMS_processed/segmentation/135/135_ins.npy'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Test cases: \"+str(HMS_data_dict_test.keys()))\n",
    "case = \"135\"\n",
    "print(\"for test case \"+str(case)+\" : \"+str(HMS_data_dict_test[case]))\n",
    "\n",
    "# you may load the image using another path\n",
    "raw_img=np.load(HMS_data_dict_test[case][\"raw\"]).astype(float)\n",
    "hand_seg=np.load(HMS_data_dict_test[case][\"ins\"]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# get gt boundary\n",
    "\n",
    "boundary_gt = np.load(HMS_data_dict_test[case][\"boundary\"]).astype(float)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feed raw image crops to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed raw img to model\n",
      "1: Transpose the image to be: [0, 1, 2]\n",
      "Progress of segment_3d_img: 0%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CellSeg3D_torch/lib/python3.8/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress of segment_3d_img: 73%\r"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# feed the raw img to the model\n",
    "print('Feed raw img to model')\n",
    "raw_img_size=raw_img.shape\n",
    "    \n",
    "seg_background_comp = np.zeros(raw_img_size)\n",
    "seg_boundary_comp = np.zeros(raw_img_size)\n",
    "\n",
    "transposes = [[0,1,2]]#,[2,0,1],[0,2,1]]\n",
    "reverse_transposes = [[0,1,2]]#,[1,2,0],[0,2,1]]\n",
    "\n",
    "for idx, transpose in enumerate(transposes):\n",
    "    print(str(idx+1)+\": Transpose the image to be: \"+str(transpose))\n",
    "    with torch.no_grad():\n",
    "        seg_img =\\\n",
    "        semantic_segment_crop_and_cat_3_channel_output_edge_gated_model(raw_img.transpose(transpose), model, device, crop_cube_size=crop_cube_size, stride=stride)\n",
    "    seg_img_background=seg_img['background']\n",
    "    seg_img_boundary=seg_img['boundary']\n",
    "    seg_img_foreground=seg_img['foreground']\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # argmax\n",
    "    print('argmax', end='\\r')\n",
    "    seg=[]\n",
    "    seg.append(seg_img_background)\n",
    "    seg.append(seg_img_boundary)\n",
    "    seg.append(seg_img_foreground)\n",
    "    seg=np.array(seg)\n",
    "    seg_argmax=np.argmax(seg, axis=0)\n",
    "    # probability map to 0 1 segment\n",
    "    seg_background=np.zeros(seg_img_background.shape)\n",
    "    seg_background[np.where(seg_argmax==0)]=1\n",
    "    seg_foreground=np.zeros(seg_img_foreground.shape)\n",
    "    seg_foreground[np.where(seg_argmax==2)]=1\n",
    "    seg_boundary=np.zeros(seg_img_boundary.shape)\n",
    "    seg_boundary[np.where(seg_argmax==1)]=1\n",
    "        \n",
    "    seg_background=seg_background.transpose(reverse_transposes[idx])\n",
    "    seg_foreground=seg_foreground.transpose(reverse_transposes[idx])\n",
    "    seg_boundary=seg_boundary.transpose(reverse_transposes[idx])\n",
    "        \n",
    "    seg_background_comp+=seg_background\n",
    "    seg_boundary_comp+=seg_boundary\n",
    "#print(\"Get model semantic seg by combination\")\n",
    "seg_background_comp = np.array(seg_background_comp>0, dtype=float)\n",
    "seg_boundary_comp = np.array(seg_boundary_comp>0, dtype=float)\n",
    "seg_foreground_comp = np.array(1 - seg_background_comp - seg_boundary_comp>0, dtype=float)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time elapsed: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# show current result\n",
    "\n",
    "N=100\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"raw_img\")\n",
    "plt.imshow(raw_img[:,:,N])\n",
    "plt.figure()\n",
    "plt.title(\"hand_seg\")\n",
    "plt.imshow(reassign(hand_seg[:,:,N]))\n",
    "plt.figure()\n",
    "plt.title(\"model_seg_foreground\")\n",
    "plt.imshow(seg_foreground_comp[:,:,N])\n",
    "plt.figure()\n",
    "plt.title(\"model_seg_boundary\")\n",
    "plt.imshow(seg_boundary_comp[:,:,N])\n",
    "plt.figure()\n",
    "plt.title(\"boundary ground truth\")\n",
    "plt.imshow(boundary_gt[:,:,N])\n",
    "plt.figure()\n",
    "plt.title(\"model_seg_background\")\n",
    "plt.imshow(seg_background_comp[:,:,N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate super vox by watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "how_close_are_the_super_vox_to_boundary=2\n",
    "min_touching_percentage=0.51\n",
    "\n",
    "seg_foreground_erosion=1-img_3d_erosion_or_expansion(1-seg_foreground_comp, kernel_size=how_close_are_the_super_vox_to_boundary+1, device=device)\n",
    "seg_foreground_super_voxel_by_ws = generate_super_vox_by_watershed(seg_foreground_erosion, connectivity=min_touching_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,7))\n",
    "plt.title(\"seg foreground erosion\")\n",
    "plt.axis('off')\n",
    "plt.imshow(seg_foreground_erosion[:,:,N])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,7))\n",
    "plt.title(\"seg foreground\")\n",
    "plt.axis('off')\n",
    "plt.imshow(seg_foreground_comp[:,:,N])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,7))\n",
    "plt.title(\"1 - seg foreground\")\n",
    "plt.axis('off')\n",
    "plt.imshow(1-seg_foreground_comp[:,:,N])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"There are \"+str(len(np.unique(seg_foreground_super_voxel_by_ws)))+\" super voxels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "super voxel clustearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cluster_super_vox=Cluster_Super_Vox(min_touching_area=min_touching_area, min_touching_percentage=min_touching_percentage)\n",
    "cluster_super_vox.fit(seg_foreground_super_voxel_by_ws)\n",
    "seg_foreground_single_cell_with_boundary = cluster_super_vox.output_3d_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete too small cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "min_cell_size_threshold=10\n",
    "seg_foreground_single_cell_with_boundary_delete_too_small = delete_too_small_cluster(seg_foreground_single_cell_with_boundary, threshold=min_cell_size_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assign boudary voxels to their nearest cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "seg_final=assign_boudary_voxels_to_cells_with_watershed(seg_foreground_single_cell_with_boundary_delete_too_small, seg_boundary_comp, seg_background_comp, compactness=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def colorful_seg(seg):\n",
    "    unique_vals, val_counts = np.unique(seg, return_counts=True)\n",
    "    \n",
    "    background_val = unique_vals[np.argsort(val_counts)[::-1][0]]\n",
    "    \n",
    "    seg_RGB = []\n",
    "    for i in range(seg.shape[0]):\n",
    "        mask_gray = cv2.normalize(src=seg[i,:,:], dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
    "        seg_slice_RGB = cv2.cvtColor(mask_gray, cv2.COLOR_GRAY2RGB)\n",
    "        seg_RGB.append(seg_slice_RGB)\n",
    "    seg_RGB = np.array(seg_RGB)\n",
    "    \n",
    "    for idx, unique_val in enumerate(unique_vals):\n",
    "        print(str(idx/len(unique_vals)), end=\"\\r\")\n",
    "        if unique_val == background_val:\n",
    "            COLOR = np.array([0,0,0], dtype=int)\n",
    "        else:\n",
    "            COLOR = np.array(np.random.choice(np.arange(256), size=3, replace=False), dtype=int)\n",
    "        \n",
    "        locs = np.where(seg==unique_val)\n",
    "        \n",
    "        for i in range(3):\n",
    "            seg_RGB[locs[0], locs[1], locs[2], i] = COLOR[i]\n",
    "        \n",
    "    return seg_RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "seg_RGB = colorful_seg(seg_final)\n",
    "hand_seg_RGB = colorful_seg(hand_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N=100\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "plt.title(\"supervoxels\")\n",
    "plt.axis('off')\n",
    "plt.imshow(seg_foreground_super_voxel_by_ws[:,:,N])#, cmap=\"gray\")\n",
    "#plt.savefig('_RGB_'+str(N)+'.png',bbox_inches='tight',dpi=fig.dpi,pad_inches=0.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N=100\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "plt.title(\"supervoxels after grouping\")\n",
    "plt.axis('off')\n",
    "plt.imshow(seg_foreground_single_cell_with_boundary[:,:,N])#, cmap=\"gray\")\n",
    "#plt.savefig('_RGB_'+str(N)+'.png',bbox_inches='tight',dpi=fig.dpi,pad_inches=0.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,7))\n",
    "plt.title(\"seg foreground\")\n",
    "plt.axis('off')\n",
    "plt.imshow(seg_foreground_comp[:,:,N])#, cmap=\"gray\")\n",
    "#plt.savefig('_RGB_'+str(N)+'.png',bbox_inches='tight',dpi=fig.dpi,pad_inches=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N=100\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "plt.title(\"model seg\")\n",
    "plt.axis('off')\n",
    "plt.imshow(seg_RGB[:,:,N,:])#, cmap=\"gray\")\n",
    "#plt.savefig('_RGB_'+str(N)+'.png',bbox_inches='tight',dpi=fig.dpi,pad_inches=0.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,7))\n",
    "plt.title(\"hand seg\")\n",
    "plt.axis('off')\n",
    "plt.imshow(hand_seg_RGB[:,:,N,:])#,"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# print(\"There are \"+str(len(np.unique(seg_final)))+\" cells in model prediction\")\n",
    "# print(\"There are \"+str(len(np.unique(hand_seg)))+\" cells in hand seg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ARI = adjusted_rand_score(hand_seg.flatten(), seg_final.flatten())\n",
    "ARE = adapted_rand_error(hand_seg.astype(int).flatten(), seg_final.astype(int).flatten())\n",
    "VOI_val = VOI(seg_final.astype(int),hand_seg.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"ARI: \"+str(ARI))\n",
    "print(\"ARE: \"+str(ARE))\n",
    "print(\"VOI: \"+str(VOI_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def img_3d_interpolate(img_3d, output_size, device = torch.device('cpu'), mode='nearest'):\n",
    "    img_3d = img_3d.reshape(1,1,img_3d.shape[0],img_3d.shape[1],img_3d.shape[2])\n",
    "    img_3d=torch.from_numpy(img_3d).float().to(device)\n",
    "    img_3d=torch.nn.functional.interpolate(img_3d, size=output_size, mode='nearest')\n",
    "    img_3d=img_3d.detach().cpu().numpy()\n",
    "    img_3d=img_3d.reshape(img_3d.shape[2],img_3d.shape[3],img_3d.shape[4])\n",
    "    \n",
    "    return img_3d\n",
    "\n",
    "scale_factor = 0.5\n",
    "org_shape = seg_final.shape\n",
    "output_size = (int(org_shape[0]*scale_factor), int(org_shape[1]*scale_factor), int(org_shape[2]*scale_factor))\n",
    "print(str(org_shape)+\" --> \"+str(output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "accuracy=IOU_and_Dice_Accuracy(img_3d_interpolate(hand_seg, output_size = output_size), img_3d_interpolate(seg_final, output_size = output_size))\n",
    "accuracy_record=accuracy.cal_accuracy_II()\n",
    "\n",
    "iou=np.array(accuracy_record[:,1]>0.7, dtype=np.float)\n",
    "print('cell count accuracy iou >0.7: '+str(sum(iou)/len(iou)))\n",
    "\n",
    "dice=np.array(accuracy_record[:,2]>0.7, dtype=np.float)\n",
    "print('cell count accuracy dice >0.7: '+str(sum(dice)/len(dice)))\n",
    "    \n",
    "iou=np.array(accuracy_record[:,1]>0.5, dtype=np.float)\n",
    "print('cell count accuracy iou >0.5: '+str(sum(iou)/len(iou)))\n",
    "\n",
    "dice=np.array(accuracy_record[:,2]>0.5, dtype=np.float)\n",
    "print('cell count accuracy dice >0.5: '+str(sum(dice)/len(dice)))\n",
    "\n",
    "print('avg iou: '+str(np.mean(accuracy_record[:,1])))\n",
    "print('avg dice: '+str(np.mean(accuracy_record[:,2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seg all imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def img_3d_interpolate(img_3d, output_size, device = torch.device('cpu'), mode='nearest'):\n",
    "    img_3d = img_3d.reshape(1,1,img_3d.shape[0],img_3d.shape[1],img_3d.shape[2])\n",
    "    img_3d=torch.from_numpy(img_3d).float().to(device)\n",
    "    img_3d=torch.nn.functional.interpolate(img_3d, size=output_size, mode='nearest')\n",
    "    img_3d=img_3d.detach().cpu().numpy()\n",
    "    img_3d=img_3d.reshape(img_3d.shape[2],img_3d.shape[3],img_3d.shape[4])\n",
    "    \n",
    "    return img_3d\n",
    "\n",
    "def pipeline(raw_img, hand_seg, model, device,\n",
    "             crop_cube_size, stride,\n",
    "             how_close_are_the_super_vox_to_boundary=2,\n",
    "             min_touching_area=30,\n",
    "             min_touching_percentage=0.51,\n",
    "             min_cell_size_threshold=1,\n",
    "             transposes = [[0,1,2]], reverse_transposes = [[0,1,2]]):\n",
    "    \n",
    "    seg_final=segment_super_vox_3_channel(raw_img, model, device,\n",
    "                                          crop_cube_size=crop_cube_size, stride=stride,\n",
    "                                          how_close_are_the_super_vox_to_boundary=how_close_are_the_super_vox_to_boundary,\n",
    "                                          min_touching_area=min_touching_area,\n",
    "                                          min_touching_percentage=min_touching_percentage,\n",
    "                                          min_cell_size_threshold=min_cell_size_threshold,\n",
    "                                          transposes = transposes, reverse_transposes = reverse_transposes)\n",
    "    \n",
    "    ari = adjusted_rand_score(hand_seg.flatten(), seg_final.flatten())\n",
    "    voi = VOI(seg_final.astype(np.int),hand_seg.astype(np.int))\n",
    "    \n",
    "    scale_factor = 0.5\n",
    "    org_shape = seg_final.shape\n",
    "    output_size = (int(org_shape[0]*scale_factor), int(org_shape[1]*scale_factor), int(org_shape[2]*scale_factor))\n",
    "    print(str(org_shape)+\" --> \"+str(output_size))\n",
    "    \n",
    "    accuracy=IOU_and_Dice_Accuracy(img_3d_interpolate(hand_seg, output_size = output_size),\n",
    "                                   img_3d_interpolate(seg_final, output_size = output_size))\n",
    "    accuracy_record=accuracy.cal_accuracy_II()\n",
    "    hand_seg_after_accuracy=accuracy.gt\n",
    "    seg_final_after_accuracy=accuracy.pred\n",
    "    \n",
    "    return accuracy_record, hand_seg_after_accuracy, seg_final_after_accuracy, ari, voi, seg_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# mass process\n",
    "seg_final_dict={}\n",
    "accuracy_record_dict = {}\n",
    "ari_dict = {}\n",
    "voi_dict = {}\n",
    "for test_file in HMS_data_dict_test.keys():\n",
    "    print(test_file)\n",
    "    raw_img=np.load(HMS_data_dict_test[test_file][\"raw\"])\n",
    "    hand_seg=np.load(HMS_data_dict_test[test_file][\"ins\"])\n",
    "    accuracy_record, hand_seg_after_accuracy, seg_final_after_accuracy, ari, voi, seg_final=\\\n",
    "    pipeline(raw_img, hand_seg, model, device,\n",
    "             crop_cube_size=64,\n",
    "             stride=32)\n",
    "    \n",
    "    seg_final_dict[test_file] = seg_final\n",
    "    accuracy_record_dict[test_file] = accuracy_record\n",
    "    ari_dict[test_file] = ari\n",
    "    voi_dict[test_file] = voi\n",
    "    \n",
    "    iou=np.array(accuracy_record[:,1]>0.7, dtype=np.float)\n",
    "    print('cell count accuracy iou >0.7: '+str(sum(iou)/len(iou)))\n",
    "\n",
    "    dice=np.array(accuracy_record[:,2]>0.7, dtype=np.float)\n",
    "    print('cell count accuracy dice >0.7: '+str(sum(dice)/len(dice)))\n",
    "    \n",
    "    iou=np.array(accuracy_record[:,1]>0.5, dtype=np.float)\n",
    "    print('cell count accuracy iou >0.5: '+str(sum(iou)/len(iou)))\n",
    "\n",
    "    dice=np.array(accuracy_record[:,2]>0.5, dtype=np.float)\n",
    "    print('cell count accuracy dice >0.5: '+str(sum(dice)/len(dice)))\n",
    "\n",
    "    print('avg iou: '+str(np.mean(accuracy_record[:,1])))\n",
    "    print('avg dice: '+str(np.mean(accuracy_record[:,2])))\n",
    "    print(\"ari: \"+str(ari))\n",
    "    print(\"voi: \"+str(voi))\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for item in seg_final_dict.keys():\n",
    "    print(item)\n",
    "    accuracy_record = accuracy_record_dict[item]\n",
    "    ari = ari_dict[item]\n",
    "    voi = voi_dict[item]\n",
    "    iou=np.array(accuracy_record[:,1]>0.7, dtype=np.float)\n",
    "    print('cell count accuracy iou >0.7: '+str(sum(iou)/len(iou)))\n",
    "\n",
    "    dice=np.array(accuracy_record[:,2]>0.7, dtype=np.float)\n",
    "    print('cell count accuracy dice >0.7: '+str(sum(dice)/len(dice)))\n",
    "    \n",
    "    iou=np.array(accuracy_record[:,1]>0.5, dtype=np.float)\n",
    "    print('cell count accuracy iou >0.5: '+str(sum(iou)/len(iou)))\n",
    "\n",
    "    dice=np.array(accuracy_record[:,2]>0.5, dtype=np.float)\n",
    "    print('cell count accuracy dice >0.5: '+str(sum(dice)/len(dice)))\n",
    "\n",
    "    print('avg iou: '+str(np.mean(accuracy_record[:,1])))\n",
    "    print('avg dice: '+str(np.mean(accuracy_record[:,2])))\n",
    "    print(\"ari: \"+str(ari))\n",
    "    print(\"voi: \"+str(voi))\n",
    "    print(\"----------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# see the difference for supervoxels\n",
    "\n",
    "how_close_are_the_super_vox_to_boundary=2\n",
    "min_touching_percentage=0.51\n",
    "\n",
    "seg_foreground_erosion=1-img_3d_erosion_or_expansion(1-seg_foreground_comp, kernel_size=how_close_are_the_super_vox_to_boundary+1, device=device)\n",
    "seg_foreground_super_voxel_by_ws = generate_super_vox_by_watershed(seg_foreground_erosion, connectivity=min_touching_area)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N=100\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "plt.title(\"supervoxels\")\n",
    "plt.axis('off')\n",
    "plt.imshow(seg_foreground_super_voxel_by_ws[:,:,N])#, cmap=\"gray\")\n",
    "#plt.savefig('_RGB_'+str(N)+'.png',bbox_inches='tight',dpi=fig.dpi,pad_inches=0.0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}