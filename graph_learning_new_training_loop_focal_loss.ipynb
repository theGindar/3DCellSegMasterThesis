{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "from func.run_pipeline_super_vox import get_outlayer_of_a_3d_shape, get_crop_by_pixel_val\n",
    "from func.ultis import load_obj\n",
    "\n",
    "from func.graph_learning import SuperVoxToNxGraph, VoxelGraphDataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# load graphs\n",
    "from func.ultis import load_obj\n",
    "\n",
    "# graphs = load_obj(\"graphs_dataset_train\")\n",
    "graphs = load_obj(\"graphs_dataset_train_with_augmentations\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "random.shuffle(graphs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CellSeg3D_torch/lib/python3.8/site-packages/dgl/backend/pytorch/tensor.py:40: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/pytorch/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  return th.as_tensor(data, dtype=dtype)\n"
     ]
    }
   ],
   "source": [
    "dataset = VoxelGraphDataset(graphs, with_edge_weights=True)\n",
    "\n",
    "g = dataset[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO probably should normalize features!!!!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ntrain_sampler = SubsetRandomSampler(torch.arange(num_train))\\ntest_sampler = SubsetRandomSampler(torch.arange(num_train, num_examples))\\n\\ntrain_dataloader = GraphDataLoader(\\n    dataset, sampler=train_sampler, batch_size=5, drop_last=False)\\n'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "num_examples = len(dataset)\n",
    "num_train = int(num_examples)\n",
    "\n",
    "\"\"\"\n",
    "train_sampler = SubsetRandomSampler(torch.arange(num_train))\n",
    "test_sampler = SubsetRandomSampler(torch.arange(num_train, num_examples))\n",
    "\n",
    "train_dataloader = GraphDataLoader(\n",
    "    dataset, sampler=train_sampler, batch_size=5, drop_last=False)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from func.graph_models import GCN, GCN_2\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "model = GCN(3, num_classes=1)\n",
    "CELoss = nn.CrossEntropyLoss()\n",
    "\n",
    "# training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "best_val_acc = 0\n",
    "\n",
    "\n",
    "\n",
    "# features = g.ndata['feat']\n",
    "# labels = g.ndata['label']\n",
    "# train_mask = g.ndata['train_mask']\n",
    "# val_mask = g.ndata['val_mask']\n",
    "\n",
    "# calculate weights for loss\n",
    "\"\"\"\n",
    "pos_weights = []\n",
    "neg_weights = []\n",
    "for graph_number in range(len(dataset)):\n",
    "    sample_graph = dataset[graph_number]\n",
    "    labels = sample_graph.ndata['label']\n",
    "    # create class weights\n",
    "    number_positives = torch.count_nonzero(labels)\n",
    "    positive_weight = 1 - (number_positives / len(labels))\n",
    "    negative_weight = 1 - positive_weight\n",
    "\n",
    "    pos_weights.append(positive_weight.item())\n",
    "    neg_weights.append(negative_weight.item())\n",
    "weights = torch.tensor([mean(neg_weights), mean(pos_weights)])\n",
    "print(f\"weights: {weights}\")\n",
    "\"\"\"\n",
    "from torchmetrics import F1Score\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "\n",
    "f1 = F1Score(num_classes=1, average='weighted')\n",
    "\n",
    "epoch_loss = []\n",
    "epoch_val_loss = []\n",
    "epoch_accuracy = []\n",
    "\n",
    "epoch_f1score = []\n",
    "epoch_f1score_val = []\n",
    "\n",
    "epoch_accuracy_val = []\n",
    "# best_val_acc = 0\n",
    "best_val_loss = 1000\n",
    "\n",
    "for e in range(500):\n",
    "    alpha = 0.25\n",
    "    # get random elements for batch\n",
    "    #graphs_numbers_list = range(0, len(dataset))\n",
    "    #rand_graph_numbers = random.sample(graphs_numbers_list, len(dataset))\n",
    "    for graph_number in range(len(dataset)):\n",
    "    #for graph_number in range(1):\n",
    "    #for graph_number in rand_graph_numbers:\n",
    "        # Forward\n",
    "        model.train()\n",
    "        sample_graph = dataset[graph_number]\n",
    "        features = sample_graph.ndata['feat']\n",
    "        labels = sample_graph.ndata['label']\n",
    "        train_mask = sample_graph.ndata['train_mask']\n",
    "        val_mask = sample_graph.ndata['val_mask']\n",
    "\n",
    "        # create class weights\n",
    "        number_positives = torch.count_nonzero(labels)\n",
    "        percentage_positives = number_positives / len(labels)\n",
    "        percentage_negatives = 1 - percentage_positives\n",
    "\n",
    "        # weights = torch.tensor([1 - percentage_negatives, 1 - percentage_positives])\n",
    "        # print(f\"weights: {weights}\")\n",
    "        # weights = torch.tensor([0.6, 0.4])\n",
    "        # print(f\"weights: {weights}\")\n",
    "        # print(f\"weights: {weights}\")\n",
    "\n",
    "        # CELoss = nn.CrossEntropyLoss(weight=weights)\n",
    "        train_mask = sample_graph.ndata['train_mask']\n",
    "        val_mask = sample_graph.ndata['val_mask']\n",
    "        logits = model(sample_graph, features)\n",
    "        model_output = torch.sigmoid(logits)\n",
    "\n",
    "        # Compute prediction\n",
    "        pred = (model_output > 0.5).type(torch.FloatTensor)\n",
    "        #print(f\"num of predicted negatives: {len(pred[pred==0])}\")\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        # loss = CELoss(logits[train_mask], labels[train_mask])\n",
    "        loss = sigmoid_focal_loss(torch.squeeze(logits[train_mask].type(torch.FloatTensor)), labels[train_mask].type(torch.FloatTensor), alpha=alpha, reduction=\"mean\")\n",
    "\n",
    "\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        #print(loss)\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "\n",
    "        train_f1_score = f1(pred[train_mask], labels[train_mask])\n",
    "        val_f1_score = f1(pred[val_mask], labels[val_mask])\n",
    "\n",
    "\n",
    "        epoch_accuracy.append(train_acc.item())\n",
    "        epoch_accuracy_val.append(val_acc.item())\n",
    "\n",
    "        epoch_f1score.append(train_f1_score.item())\n",
    "        epoch_f1score_val.append(val_f1_score.item())\n",
    "\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(sample_graph, features)\n",
    "            # val_loss = CELoss(logits[val_mask], labels[val_mask])\n",
    "            val_loss = sigmoid_focal_loss(torch.squeeze(logits[val_mask].type(torch.FloatTensor)), labels[val_mask].type(torch.FloatTensor), alpha=alpha, reduction=\"mean\")\n",
    "            epoch_val_loss.append(val_loss.item())\n",
    "        model.train()\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        print('In epoch {}, loss: {:.5f}, val loss: {:.5f}, accuracy: {:.3f}, val accuracy: {:.3f}, f1score: {:.3f}, val f1score: {:.3f}'.format(\n",
    "            e, mean(epoch_loss), mean(epoch_val_loss), mean(epoch_accuracy), mean(epoch_accuracy_val), mean(epoch_f1score), mean(epoch_f1score_val)))\n",
    "\n",
    "        #if mean(epoch_accuracy_val) >= best_val_acc:\n",
    "        if mean(epoch_val_loss) <= best_val_loss:\n",
    "            print(\"new best val loss\")\n",
    "            torch.save(model.state_dict(), \"output/graph_model.pt\")\n",
    "            best_val_loss = mean(epoch_val_loss)\n",
    "        epoch_loss = []\n",
    "        epoch_val_loss = []\n",
    "        epoch_accuracy = []\n",
    "\n",
    "        epoch_accuracy_val = []\n",
    "        epoch_f1score_val = []\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CellSeg3D_torch/lib/python3.8/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.05634, val loss: 0.06731, accuracy: 0.785, val accuracy: 0.710, f1score: 0.876, val f1score: 0.818\n",
      "new best val loss\n",
      "In epoch 5, loss: 0.05534, val loss: 0.06612, accuracy: 0.770, val accuracy: 0.695, f1score: 0.871, val f1score: 0.811\n",
      "new best val loss\n",
      "In epoch 10, loss: 0.05499, val loss: 0.06560, accuracy: 0.759, val accuracy: 0.685, f1score: 0.868, val f1score: 0.807\n",
      "new best val loss\n",
      "In epoch 15, loss: 0.05488, val loss: 0.06546, accuracy: 0.756, val accuracy: 0.682, f1score: 0.866, val f1score: 0.805\n",
      "new best val loss\n",
      "In epoch 20, loss: 0.05485, val loss: 0.06541, accuracy: 0.756, val accuracy: 0.681, f1score: 0.866, val f1score: 0.805\n",
      "new best val loss\n",
      "In epoch 25, loss: 0.05482, val loss: 0.06538, accuracy: 0.756, val accuracy: 0.681, f1score: 0.865, val f1score: 0.805\n",
      "new best val loss\n",
      "In epoch 30, loss: 0.05480, val loss: 0.06535, accuracy: 0.755, val accuracy: 0.679, f1score: 0.865, val f1score: 0.805\n",
      "new best val loss\n",
      "In epoch 35, loss: 0.05477, val loss: 0.06530, accuracy: 0.755, val accuracy: 0.678, f1score: 0.865, val f1score: 0.804\n",
      "new best val loss\n",
      "In epoch 40, loss: 0.05474, val loss: 0.06528, accuracy: 0.754, val accuracy: 0.677, f1score: 0.865, val f1score: 0.804\n",
      "new best val loss\n",
      "In epoch 45, loss: 0.05473, val loss: 0.06529, accuracy: 0.754, val accuracy: 0.678, f1score: 0.865, val f1score: 0.804\n",
      "In epoch 50, loss: 0.05470, val loss: 0.06526, accuracy: 0.754, val accuracy: 0.677, f1score: 0.864, val f1score: 0.804\n",
      "new best val loss\n",
      "In epoch 55, loss: 0.05471, val loss: 0.06525, accuracy: 0.754, val accuracy: 0.677, f1score: 0.864, val f1score: 0.804\n",
      "new best val loss\n",
      "In epoch 60, loss: 0.05469, val loss: 0.06524, accuracy: 0.753, val accuracy: 0.677, f1score: 0.864, val f1score: 0.804\n",
      "new best val loss\n",
      "In epoch 65, loss: 0.05469, val loss: 0.06523, accuracy: 0.753, val accuracy: 0.676, f1score: 0.864, val f1score: 0.803\n",
      "new best val loss\n",
      "In epoch 70, loss: 0.05468, val loss: 0.06521, accuracy: 0.754, val accuracy: 0.676, f1score: 0.864, val f1score: 0.804\n",
      "new best val loss\n",
      "In epoch 75, loss: 0.05465, val loss: 0.06523, accuracy: 0.753, val accuracy: 0.675, f1score: 0.864, val f1score: 0.803\n",
      "In epoch 80, loss: 0.05464, val loss: 0.06521, accuracy: 0.752, val accuracy: 0.676, f1score: 0.864, val f1score: 0.803\n",
      "new best val loss\n",
      "In epoch 85, loss: 0.05464, val loss: 0.06518, accuracy: 0.751, val accuracy: 0.674, f1score: 0.864, val f1score: 0.802\n",
      "new best val loss\n",
      "In epoch 90, loss: 0.05460, val loss: 0.06516, accuracy: 0.750, val accuracy: 0.673, f1score: 0.864, val f1score: 0.801\n",
      "new best val loss\n",
      "In epoch 95, loss: 0.05460, val loss: 0.06515, accuracy: 0.750, val accuracy: 0.673, f1score: 0.864, val f1score: 0.801\n",
      "new best val loss\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 52>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    100\u001B[0m val_acc \u001B[38;5;241m=\u001B[39m (pred[val_mask] \u001B[38;5;241m==\u001B[39m labels[val_mask])\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mmean()\n\u001B[1;32m    102\u001B[0m train_f1_score \u001B[38;5;241m=\u001B[39m f1(pred[train_mask], labels[train_mask])\n\u001B[0;32m--> 103\u001B[0m val_f1_score \u001B[38;5;241m=\u001B[39m \u001B[43mf1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred\u001B[49m\u001B[43m[\u001B[49m\u001B[43mval_mask\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m[\u001B[49m\u001B[43mval_mask\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    106\u001B[0m epoch_accuracy\u001B[38;5;241m.\u001B[39mappend(train_acc\u001B[38;5;241m.\u001B[39mitem())\n\u001B[1;32m    107\u001B[0m epoch_accuracy_val\u001B[38;5;241m.\u001B[39mappend(val_acc\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[0;32m/opt/anaconda3/envs/CellSeg3D_torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/anaconda3/envs/CellSeg3D_torch/lib/python3.8/site-packages/torchmetrics/metric.py:264\u001B[0m, in \u001B[0;36mMetric.forward\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreset()\n\u001B[1;32m    263\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 264\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;66;03m# restore context\u001B[39;00m\n\u001B[1;32m    267\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m attr, val \u001B[38;5;129;01min\u001B[39;00m cache\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[0;32m/opt/anaconda3/envs/CellSeg3D_torch/lib/python3.8/site-packages/torchmetrics/metric.py:440\u001B[0m, in \u001B[0;36mMetric._wrap_compute.<locals>.wrapped_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    432\u001B[0m \u001B[38;5;66;03m# compute relies on the sync context manager to gather the states across processes and apply reduction\u001B[39;00m\n\u001B[1;32m    433\u001B[0m \u001B[38;5;66;03m# if synchronization happened, the current rank accumulated states will be restored to keep\u001B[39;00m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;66;03m# accumulation going if ``should_unsync=True``,\u001B[39;00m\n\u001B[1;32m    435\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msync_context(\n\u001B[1;32m    436\u001B[0m     dist_sync_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdist_sync_fn,  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    437\u001B[0m     should_sync\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_to_sync,\n\u001B[1;32m    438\u001B[0m     should_unsync\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_unsync,\n\u001B[1;32m    439\u001B[0m ):\n\u001B[0;32m--> 440\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    441\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_computed \u001B[38;5;241m=\u001B[39m _squeeze_if_scalar(value)\n\u001B[1;32m    443\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_computed\n",
      "File \u001B[0;32m/opt/anaconda3/envs/CellSeg3D_torch/lib/python3.8/site-packages/torchmetrics/classification/f_beta.py:163\u001B[0m, in \u001B[0;36mFBetaScore.compute\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;124;03m\"\"\"Computes f-beta over state.\"\"\"\u001B[39;00m\n\u001B[1;32m    162\u001B[0m tp, fp, tn, fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_final_stats()\n\u001B[0;32m--> 163\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_fbeta_compute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbeta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maverage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmdmc_reduce\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/CellSeg3D_torch/lib/python3.8/site-packages/torchmetrics/functional/classification/f_beta.py:72\u001B[0m, in \u001B[0;36m_fbeta_compute\u001B[0;34m(tp, fp, tn, fn, beta, ignore_index, average, mdmc_average)\u001B[0m\n\u001B[1;32m     70\u001B[0m     recall \u001B[38;5;241m=\u001B[39m _safe_divide(tp[mask]\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39mfloat(), (tp[mask] \u001B[38;5;241m+\u001B[39m fn[mask])\u001B[38;5;241m.\u001B[39msum())\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 72\u001B[0m     precision \u001B[38;5;241m=\u001B[39m _safe_divide(tp\u001B[38;5;241m.\u001B[39mfloat(), \u001B[43mtp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mfp\u001B[49m)\n\u001B[1;32m     73\u001B[0m     recall \u001B[38;5;241m=\u001B[39m _safe_divide(tp\u001B[38;5;241m.\u001B[39mfloat(), tp \u001B[38;5;241m+\u001B[39m fn)\n\u001B[1;32m     75\u001B[0m num \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m beta\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m*\u001B[39m precision \u001B[38;5;241m*\u001B[39m recall\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "from func.graph_models import GCN, GCN_2\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import dgl\n",
    "\n",
    "model = GCN(1, num_classes=2)\n",
    "CELoss = nn.CrossEntropyLoss()\n",
    "\n",
    "# training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "best_val_acc = 0\n",
    "\n",
    "\n",
    "\n",
    "# features = g.ndata['feat']\n",
    "# labels = g.ndata['label']\n",
    "# train_mask = g.ndata['train_mask']\n",
    "# val_mask = g.ndata['val_mask']\n",
    "\n",
    "# calculate weights for loss\n",
    "\"\"\"\n",
    "pos_weights = []\n",
    "neg_weights = []\n",
    "for graph_number in range(len(dataset)):\n",
    "    sample_graph = dataset[graph_number]\n",
    "    labels = sample_graph.ndata['label']\n",
    "    # create class weights\n",
    "    number_positives = torch.count_nonzero(labels)\n",
    "    positive_weight = 1 - (number_positives / len(labels))\n",
    "    negative_weight = 1 - positive_weight\n",
    "\n",
    "    pos_weights.append(positive_weight.item())\n",
    "    neg_weights.append(negative_weight.item())\n",
    "weights = torch.tensor([mean(neg_weights), mean(pos_weights)])\n",
    "print(f\"weights: {weights}\")\n",
    "\"\"\"\n",
    "\n",
    "# build one big graph\n",
    "graphs_list = []\n",
    "for i in range(len(dataset)):\n",
    "    graphs_list.append(dataset[i])\n",
    "\n",
    "large_g = dgl.batch(graphs_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "epoch_loss = []\n",
    "epoch_accuracy = []\n",
    "for e in range(1000):\n",
    "    # get random elements for batch\n",
    "    #graphs_numbers_list = range(0, len(dataset))\n",
    "    #rand_graph_numbers = random.sample(graphs_numbers_list, len(dataset))\n",
    "    sample_graph = large_g\n",
    "    features = sample_graph.ndata['feat']\n",
    "    labels = sample_graph.ndata['label']\n",
    "\n",
    "    # create class weights\n",
    "    number_positives = torch.count_nonzero(labels)\n",
    "    percentage_positives = number_positives / len(labels)\n",
    "    percentage_negatives = 1 - percentage_positives\n",
    "\n",
    "    weights = torch.tensor([1 - percentage_negatives, 1 - percentage_positives])\n",
    "    #weights = torch.tensor([0.95, 0.05])\n",
    "    #print(weights)\n",
    "\n",
    "    CELoss = nn.CrossEntropyLoss(weight=weights)\n",
    "    #train_mask = sample_graph.ndata['train_mask']\n",
    "    #val_mask = sample_graph.ndata['val_mask']\n",
    "    logits = model(sample_graph, features)\n",
    "\n",
    "    # Compute prediction\n",
    "    pred = logits.argmax(1)\n",
    "\n",
    "    # Compute loss\n",
    "    # Note that you should only compute the losses of the nodes in the training set.\n",
    "    loss = CELoss(logits, labels)\n",
    "    epoch_loss.append(loss.item())\n",
    "    #print(loss)\n",
    "    # Compute accuracy on training/validation/test\n",
    "    train_acc = (pred == labels).float().mean()\n",
    "    epoch_accuracy.append(train_acc.item())\n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        print('In epoch {}, loss: {:.3f}, accuracy: {:.3f}'.format(\n",
    "            e, mean(epoch_loss), mean(epoch_accuracy)))\n",
    "        epoch_loss = []\n",
    "        epoch_accuracy = []\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train on GPU\n",
    "g = g.to('cuda')\n",
    "model = GCN(1, 16, dataset.num_classes).to('cuda')\n",
    "train(g, model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "g.ndata['feat'].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(g, g.ndata['feat']).argmax(1).numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.unique(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = g.ndata['label']\n",
    "np.unique(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(labels[labels==1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(labels[labels==0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(predictions[predictions==1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(predictions[predictions==0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_model = GCN(3, num_classes=2)\n",
    "checkpoint_graph = 'output/graph_model.pt'\n",
    "graph_model.load_state_dict(torch.load(checkpoint_graph))\n",
    "graph_model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = graph_model(g, g.ndata['feat']).argmax(1).numpy()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "len(predictions[predictions==1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(predictions[predictions==0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}