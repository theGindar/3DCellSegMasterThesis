{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "from func.run_pipeline_super_vox import get_outlayer_of_a_3d_shape, get_crop_by_pixel_val\n",
    "from func.ultis import load_obj\n",
    "\n",
    "from func.graph_learning import SuperVoxToNxGraph, VoxelGraphDataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# load graphs\n",
    "from func.ultis import load_obj\n",
    "\n",
    "graphs = load_obj(\"graphs_dataset_train\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "dataset = VoxelGraphDataset(graphs)\n",
    "\n",
    "g = dataset[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO probably should normalize features!!!!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "num_examples = len(dataset)\n",
    "num_train = int(num_examples)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(torch.arange(num_train))\n",
    "test_sampler = SubsetRandomSampler(torch.arange(num_train, num_examples))\n",
    "\n",
    "train_dataloader = GraphDataLoader(\n",
    "    dataset, sampler=train_sampler, batch_size=5, drop_last=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from func.graph_models import GCN, GCN_2\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "model = GCN(1, num_classes=2)\n",
    "CELoss = nn.CrossEntropyLoss()\n",
    "\n",
    "# training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "best_val_acc = 0\n",
    "\n",
    "\n",
    "\n",
    "# features = g.ndata['feat']\n",
    "# labels = g.ndata['label']\n",
    "# train_mask = g.ndata['train_mask']\n",
    "# val_mask = g.ndata['val_mask']\n",
    "\n",
    "# calculate weights for loss\n",
    "\"\"\"\n",
    "pos_weights = []\n",
    "neg_weights = []\n",
    "for graph_number in range(len(dataset)):\n",
    "    sample_graph = dataset[graph_number]\n",
    "    labels = sample_graph.ndata['label']\n",
    "    # create class weights\n",
    "    number_positives = torch.count_nonzero(labels)\n",
    "    positive_weight = 1 - (number_positives / len(labels))\n",
    "    negative_weight = 1 - positive_weight\n",
    "\n",
    "    pos_weights.append(positive_weight.item())\n",
    "    neg_weights.append(negative_weight.item())\n",
    "weights = torch.tensor([mean(neg_weights), mean(pos_weights)])\n",
    "print(f\"weights: {weights}\")\n",
    "\"\"\"\n",
    "epoch_loss = []\n",
    "epoch_accuracy = []\n",
    "\n",
    "epoch_accuracy_val = []\n",
    "for e in range(2000):\n",
    "    # get random elements for batch\n",
    "    #graphs_numbers_list = range(0, len(dataset))\n",
    "    #rand_graph_numbers = random.sample(graphs_numbers_list, len(dataset))\n",
    "    for graph_number in range(len(dataset)):\n",
    "    #for graph_number in range(1):\n",
    "    #for graph_number in rand_graph_numbers:\n",
    "        # Forward\n",
    "        sample_graph = dataset[graph_number]\n",
    "        features = sample_graph.ndata['feat']\n",
    "        labels = sample_graph.ndata['label']\n",
    "        train_mask = sample_graph.ndata['train_mask']\n",
    "        val_mask = sample_graph.ndata['val_mask']\n",
    "\n",
    "        # create class weights\n",
    "        number_positives = torch.count_nonzero(labels)\n",
    "        percentage_positives = number_positives / len(labels)\n",
    "        percentage_negatives = 1 - percentage_positives\n",
    "\n",
    "        weights = torch.tensor([1 - percentage_negatives, 1 - percentage_positives])\n",
    "        #weights = torch.tensor([0.95, 0.05])\n",
    "        #print(weights)\n",
    "\n",
    "        CELoss = nn.CrossEntropyLoss(weight=weights)\n",
    "        train_mask = sample_graph.ndata['train_mask']\n",
    "        val_mask = sample_graph.ndata['val_mask']\n",
    "        logits = model(sample_graph, features)\n",
    "\n",
    "        # Compute prediction\n",
    "        pred = logits.argmax(1)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        loss = CELoss(logits[train_mask], labels[train_mask])\n",
    "        epoch_loss.append(loss.item())\n",
    "        #print(loss)\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        epoch_accuracy.append(train_acc.item())\n",
    "        epoch_accuracy_val.append(val_acc.item())\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        print('In epoch {}, loss: {:.3f}, accuracy: {:.3f}, val accuracy: {:.3f}'.format(\n",
    "            e, mean(epoch_loss), mean(epoch_accuracy), mean(epoch_accuracy_val)))\n",
    "        epoch_loss = []\n",
    "        epoch_accuracy = []\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CellSeg3D_torch/lib/python3.8/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.715, accuracy: 0.464, val accuracy: 0.470\n",
      "In epoch 20, loss: 0.692, accuracy: 0.896, val accuracy: 0.858\n",
      "In epoch 40, loss: 0.692, accuracy: 0.934, val accuracy: 0.884\n",
      "In epoch 60, loss: 0.692, accuracy: 0.933, val accuracy: 0.893\n",
      "In epoch 80, loss: 0.692, accuracy: 0.930, val accuracy: 0.896\n",
      "In epoch 100, loss: 0.692, accuracy: 0.899, val accuracy: 0.893\n",
      "In epoch 120, loss: 0.692, accuracy: 0.885, val accuracy: 0.888\n",
      "In epoch 140, loss: 0.692, accuracy: 0.883, val accuracy: 0.885\n",
      "In epoch 160, loss: 0.692, accuracy: 0.835, val accuracy: 0.877\n",
      "In epoch 180, loss: 0.692, accuracy: 0.825, val accuracy: 0.869\n",
      "In epoch 200, loss: 0.692, accuracy: 0.829, val accuracy: 0.864\n",
      "In epoch 220, loss: 0.692, accuracy: 0.828, val accuracy: 0.859\n",
      "In epoch 240, loss: 0.692, accuracy: 0.821, val accuracy: 0.855\n",
      "In epoch 260, loss: 0.692, accuracy: 0.832, val accuracy: 0.852\n",
      "In epoch 280, loss: 0.692, accuracy: 0.835, val accuracy: 0.849\n",
      "In epoch 300, loss: 0.691, accuracy: 0.822, val accuracy: 0.847\n",
      "In epoch 320, loss: 0.692, accuracy: 0.830, val accuracy: 0.845\n",
      "In epoch 340, loss: 0.691, accuracy: 0.825, val accuracy: 0.842\n",
      "In epoch 360, loss: 0.691, accuracy: 0.826, val accuracy: 0.841\n",
      "In epoch 380, loss: 0.691, accuracy: 0.852, val accuracy: 0.840\n",
      "In epoch 400, loss: 0.691, accuracy: 0.832, val accuracy: 0.839\n",
      "In epoch 420, loss: 0.692, accuracy: 0.834, val accuracy: 0.838\n",
      "In epoch 440, loss: 0.691, accuracy: 0.821, val accuracy: 0.836\n",
      "In epoch 460, loss: 0.691, accuracy: 0.835, val accuracy: 0.836\n",
      "In epoch 480, loss: 0.691, accuracy: 0.821, val accuracy: 0.834\n",
      "In epoch 500, loss: 0.691, accuracy: 0.824, val accuracy: 0.833\n",
      "In epoch 520, loss: 0.691, accuracy: 0.833, val accuracy: 0.832\n",
      "In epoch 540, loss: 0.691, accuracy: 0.825, val accuracy: 0.831\n",
      "In epoch 560, loss: 0.691, accuracy: 0.830, val accuracy: 0.831\n",
      "In epoch 580, loss: 0.691, accuracy: 0.822, val accuracy: 0.830\n",
      "In epoch 600, loss: 0.691, accuracy: 0.829, val accuracy: 0.829\n",
      "In epoch 620, loss: 0.691, accuracy: 0.826, val accuracy: 0.829\n",
      "In epoch 640, loss: 0.691, accuracy: 0.838, val accuracy: 0.828\n",
      "In epoch 660, loss: 0.691, accuracy: 0.823, val accuracy: 0.828\n",
      "In epoch 680, loss: 0.691, accuracy: 0.841, val accuracy: 0.828\n",
      "In epoch 700, loss: 0.691, accuracy: 0.827, val accuracy: 0.827\n",
      "In epoch 720, loss: 0.691, accuracy: 0.835, val accuracy: 0.827\n",
      "In epoch 740, loss: 0.691, accuracy: 0.836, val accuracy: 0.827\n",
      "In epoch 760, loss: 0.691, accuracy: 0.831, val accuracy: 0.826\n",
      "In epoch 780, loss: 0.691, accuracy: 0.837, val accuracy: 0.826\n",
      "In epoch 800, loss: 0.691, accuracy: 0.829, val accuracy: 0.826\n",
      "In epoch 820, loss: 0.691, accuracy: 0.827, val accuracy: 0.825\n",
      "In epoch 840, loss: 0.691, accuracy: 0.819, val accuracy: 0.825\n",
      "In epoch 860, loss: 0.691, accuracy: 0.830, val accuracy: 0.824\n",
      "In epoch 880, loss: 0.691, accuracy: 0.834, val accuracy: 0.824\n",
      "In epoch 900, loss: 0.691, accuracy: 0.833, val accuracy: 0.824\n",
      "In epoch 920, loss: 0.691, accuracy: 0.835, val accuracy: 0.824\n",
      "In epoch 940, loss: 0.691, accuracy: 0.833, val accuracy: 0.823\n",
      "In epoch 960, loss: 0.691, accuracy: 0.833, val accuracy: 0.823\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 40>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     64\u001B[0m train_mask \u001B[38;5;241m=\u001B[39m sample_graph\u001B[38;5;241m.\u001B[39mndata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_mask\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     65\u001B[0m val_mask \u001B[38;5;241m=\u001B[39m sample_graph\u001B[38;5;241m.\u001B[39mndata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_mask\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m---> 66\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;66;03m# Compute prediction\u001B[39;00m\n\u001B[1;32m     69\u001B[0m pred \u001B[38;5;241m=\u001B[39m logits\u001B[38;5;241m.\u001B[39margmax(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/CellSeg3D_torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Documents/GitHub/3DCellSegMasterThesis/func/graph_models.py:17\u001B[0m, in \u001B[0;36mGCN.forward\u001B[0;34m(self, g, in_feat)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, g, in_feat):\n\u001B[0;32m---> 17\u001B[0m     h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43min_feat\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m     h \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(h)\n\u001B[1;32m     19\u001B[0m     h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(h)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/CellSeg3D_torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/anaconda3/envs/CellSeg3D_torch/lib/python3.8/site-packages/dgl/nn/pytorch/conv/graphconv.py:380\u001B[0m, in \u001B[0;36mGraphConv.forward\u001B[0;34m(self, graph, feat, weight, edge_weight)\u001B[0m\n\u001B[1;32m    378\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m graph\u001B[38;5;241m.\u001B[39mlocal_scope():\n\u001B[1;32m    379\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_allow_zero_in_degree:\n\u001B[0;32m--> 380\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (\u001B[43mgraph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_degrees\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39many():\n\u001B[1;32m    381\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m DGLError(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThere are 0-in-degree nodes in the graph, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    382\u001B[0m                            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput for those nodes will be invalid. \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    383\u001B[0m                            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThis is harmful for some applications, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    388\u001B[0m                            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mto be `True` when constructing this module will \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    389\u001B[0m                            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msuppress the check and let the code run.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    390\u001B[0m     aggregate_fn \u001B[38;5;241m=\u001B[39m fn\u001B[38;5;241m.\u001B[39mcopy_src(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mh\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mm\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/CellSeg3D_torch/lib/python3.8/site-packages/dgl/heterograph.py:3423\u001B[0m, in \u001B[0;36mDGLHeteroGraph.in_degrees\u001B[0;34m(self, v, etype)\u001B[0m\n\u001B[1;32m   3421\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_all(v):\n\u001B[1;32m   3422\u001B[0m     v \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdstnodes(dsttype)\n\u001B[0;32m-> 3423\u001B[0m v_tensor \u001B[38;5;241m=\u001B[39m \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3424\u001B[0m deg \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_graph\u001B[38;5;241m.\u001B[39min_degrees(etid, v_tensor)\n\u001B[1;32m   3425\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(v, numbers\u001B[38;5;241m.\u001B[39mIntegral):\n",
      "File \u001B[0;32m/opt/anaconda3/envs/CellSeg3D_torch/lib/python3.8/site-packages/dgl/utils/checks.py:31\u001B[0m, in \u001B[0;36mprepare_tensor\u001B[0;34m(g, data, name)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124;03m\"\"\"Convert the data to ID tensor and check its ID type and context.\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \n\u001B[1;32m     11\u001B[0m \u001B[38;5;124;03mIf the data is already in tensor type, raise error if its ID type\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;124;03m    Data in tensor object.\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m F\u001B[38;5;241m.\u001B[39mis_tensor(data):\n\u001B[0;32m---> 31\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m F\u001B[38;5;241m.\u001B[39mdtype(data) \u001B[38;5;241m!=\u001B[39m \u001B[43mg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43midtype\u001B[49m \u001B[38;5;129;01mor\u001B[39;00m F\u001B[38;5;241m.\u001B[39mcontext(data) \u001B[38;5;241m!=\u001B[39m g\u001B[38;5;241m.\u001B[39mdevice:\n\u001B[1;32m     32\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m DGLError(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpect argument \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m to have data type \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m and device \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     33\u001B[0m                        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontext \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. But got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m     34\u001B[0m                            name, g\u001B[38;5;241m.\u001B[39midtype, g\u001B[38;5;241m.\u001B[39mdevice, F\u001B[38;5;241m.\u001B[39mdtype(data), F\u001B[38;5;241m.\u001B[39mcontext(data)))\n\u001B[1;32m     35\u001B[0m     ret \u001B[38;5;241m=\u001B[39m data\n",
      "File \u001B[0;32m/opt/anaconda3/envs/CellSeg3D_torch/lib/python3.8/site-packages/dgl/heterograph.py:2591\u001B[0m, in \u001B[0;36mDGLHeteroGraph.idtype\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2559\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m   2560\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21midtype\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m   2561\u001B[0m     \u001B[38;5;124;03m\"\"\"The data type for storing the structure-related graph information\u001B[39;00m\n\u001B[1;32m   2562\u001B[0m \u001B[38;5;124;03m    such as node and edge IDs.\u001B[39;00m\n\u001B[1;32m   2563\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2589\u001B[0m \u001B[38;5;124;03m    int\u001B[39;00m\n\u001B[1;32m   2590\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2591\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(F, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/CellSeg3D_torch/lib/python3.8/site-packages/dgl/heterograph_index.py:159\u001B[0m, in \u001B[0;36mHeteroGraphIndex.dtype\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    156\u001B[0m     _CAPI_DGLHeteroClear(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    157\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cache\u001B[38;5;241m.\u001B[39mclear()\n\u001B[0;32m--> 159\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdtype\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    161\u001B[0m     \u001B[38;5;124;03m\"\"\"Return the data type of this graph index.\u001B[39;00m\n\u001B[1;32m    162\u001B[0m \n\u001B[1;32m    163\u001B[0m \u001B[38;5;124;03m    Returns\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    166\u001B[0m \u001B[38;5;124;03m        The data type of the graph.\u001B[39;00m\n\u001B[1;32m    167\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    168\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _CAPI_DGLHeteroDataType(\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "from func.graph_models import GCN, GCN_2\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import dgl\n",
    "\n",
    "model = GCN(1, num_classes=2)\n",
    "CELoss = nn.CrossEntropyLoss()\n",
    "\n",
    "# training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "best_val_acc = 0\n",
    "\n",
    "\n",
    "\n",
    "# features = g.ndata['feat']\n",
    "# labels = g.ndata['label']\n",
    "# train_mask = g.ndata['train_mask']\n",
    "# val_mask = g.ndata['val_mask']\n",
    "\n",
    "# calculate weights for loss\n",
    "\"\"\"\n",
    "pos_weights = []\n",
    "neg_weights = []\n",
    "for graph_number in range(len(dataset)):\n",
    "    sample_graph = dataset[graph_number]\n",
    "    labels = sample_graph.ndata['label']\n",
    "    # create class weights\n",
    "    number_positives = torch.count_nonzero(labels)\n",
    "    positive_weight = 1 - (number_positives / len(labels))\n",
    "    negative_weight = 1 - positive_weight\n",
    "\n",
    "    pos_weights.append(positive_weight.item())\n",
    "    neg_weights.append(negative_weight.item())\n",
    "weights = torch.tensor([mean(neg_weights), mean(pos_weights)])\n",
    "print(f\"weights: {weights}\")\n",
    "\"\"\"\n",
    "\n",
    "# build one big graph\n",
    "graphs_list = []\n",
    "for i in range(len(dataset)):\n",
    "    graphs_list.append(dataset[i])\n",
    "\n",
    "large_g = dgl.batch(graphs_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "epoch_loss = []\n",
    "epoch_accuracy = []\n",
    "for e in range(1000):\n",
    "    # get random elements for batch\n",
    "    #graphs_numbers_list = range(0, len(dataset))\n",
    "    #rand_graph_numbers = random.sample(graphs_numbers_list, len(dataset))\n",
    "    sample_graph = large_g\n",
    "    features = sample_graph.ndata['feat']\n",
    "    labels = sample_graph.ndata['label']\n",
    "\n",
    "    # create class weights\n",
    "    number_positives = torch.count_nonzero(labels)\n",
    "    percentage_positives = number_positives / len(labels)\n",
    "    percentage_negatives = 1 - percentage_positives\n",
    "\n",
    "    weights = torch.tensor([1 - percentage_negatives, 1 - percentage_positives])\n",
    "    #weights = torch.tensor([0.95, 0.05])\n",
    "    #print(weights)\n",
    "\n",
    "    CELoss = nn.CrossEntropyLoss(weight=weights)\n",
    "    #train_mask = sample_graph.ndata['train_mask']\n",
    "    #val_mask = sample_graph.ndata['val_mask']\n",
    "    logits = model(sample_graph, features)\n",
    "\n",
    "    # Compute prediction\n",
    "    pred = logits.argmax(1)\n",
    "\n",
    "    # Compute loss\n",
    "    # Note that you should only compute the losses of the nodes in the training set.\n",
    "    loss = CELoss(logits, labels)\n",
    "    epoch_loss.append(loss.item())\n",
    "    #print(loss)\n",
    "    # Compute accuracy on training/validation/test\n",
    "    train_acc = (pred == labels).float().mean()\n",
    "    epoch_accuracy.append(train_acc.item())\n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        print('In epoch {}, loss: {:.3f}, accuracy: {:.3f}'.format(\n",
    "            e, mean(epoch_loss), mean(epoch_accuracy)))\n",
    "        epoch_loss = []\n",
    "        epoch_accuracy = []\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train on GPU\n",
    "g = g.to('cuda')\n",
    "model = GCN(1, 16, dataset.num_classes).to('cuda')\n",
    "train(g, model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "Graph(num_nodes=1405, num_edges=5327,\n      ndata_schemes={'feat': Scheme(shape=(1,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool)}\n      edata_schemes={'weight': Scheme(shape=(1,), dtype=torch.float64)})"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1405, 1])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "g.ndata['feat'].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "\n",
    "predictions = model(g, g.ndata['feat']).argmax(1).numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = g.ndata['label']\n",
    "np.unique(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(labels[labels==1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(labels[labels==0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}