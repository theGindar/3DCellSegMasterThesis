{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import edt\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from skimage.metrics import adapted_rand_error\n",
    "\n",
    "import torch\n",
    "from torch import from_numpy as from_numpy\n",
    "from torchsummary import summary\n",
    "\n",
    "from func.run_pipeline_super_vox import segment_super_vox_3_channel, semantic_segment_crop_and_cat_3_channel_output, img_3d_erosion_or_expansion, \\\n",
    "generate_super_vox_by_watershed, get_outlayer_of_a_3d_shape, get_crop_by_pixel_val, Cluster_Super_Vox, assign_boudary_voxels_to_cells_with_watershed, \\\n",
    "delete_too_small_cluster, reassign\n",
    "from func.run_pipeline import segment, assign_boudary_voxels_to_cells, dbscan_of_seg, semantic_segment_crop_and_cat\n",
    "from func.cal_accuracy import IOU_and_Dice_Accuracy, VOI\n",
    "from func.network import VoxResNet, CellSegNet_basic_lite\n",
    "from func.unet_3d_basic import UNet3D_basic\n",
    "from func.ultis import save_obj, load_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "CellSegNet_basic_lite(\n  (conv1): Conv3d(1, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n  (conv2): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  (bnorm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv3): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n  (resmodule1): ResModule(\n    (batchnorm_module): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv_module): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  )\n  (conv4): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n  (resmodule2): ResModule(\n    (batchnorm_module): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv_module): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  )\n  (conv5): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n  (resmodule3): ResModule(\n    (batchnorm_module): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv_module): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  )\n  (deconv1): ConvTranspose3d(64, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n  (bnorm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (deconv2): ConvTranspose3d(64, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n  (bnorm3): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (deconv3): ConvTranspose3d(64, 32, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n  (bnorm4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv6): Conv3d(32, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HMS_data_dict = load_obj(\"dataset_info/HMS_dataset_info\")\n",
    "HMS_data_dict_train = HMS_data_dict[\"train\"]\n",
    "\n",
    "model=CellSegNet_basic_lite(input_channel=1, n_classes=3, output_func = \"softmax\")\n",
    "load_path='output/model_HMS_delete_fake_cells.pkl'\n",
    "checkpoint = torch.load(load_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# we do not input the whole raw image to the model one time but input raw image crops\n",
    "crop_cube_size=64\n",
    "stride=32\n",
    "\n",
    "# hyperparameter for TASCAN, min touching area of two super pixels if they belong to the same cell\n",
    "min_touching_area=30"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "data/CellSeg_dataset/HMS_processed/raw/95.npy\n",
      "Feed raw img to model\n",
      "1: Transpose the image to be: [0, 1, 2]\n",
      "Progress of segment_3d_img: 44%\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [25]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mstr\u001B[39m(idx\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m: Transpose the image to be: \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(transpose))\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m     27\u001B[0m     seg_img\u001B[38;5;241m=\u001B[39m\\\n\u001B[0;32m---> 28\u001B[0m     \u001B[43msemantic_segment_crop_and_cat_3_channel_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_img\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtranspose\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcrop_cube_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcrop_cube_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstride\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m seg_img_background\u001B[38;5;241m=\u001B[39mseg_img[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbackground\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     30\u001B[0m seg_img_boundary\u001B[38;5;241m=\u001B[39mseg_img[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mboundary\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m~/Documents/GitHub/3DCellSegMasterThesis/func/run_pipeline_super_vox.py:309\u001B[0m, in \u001B[0;36msemantic_segment_crop_and_cat_3_channel_output\u001B[0;34m(raw_img, model, device, crop_cube_size, stride)\u001B[0m\n\u001B[1;32m    306\u001B[0m seg_background_temp[seg_log_crop\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m=\u001B[39m(seg_crop_output_np_bg[seg_log_crop\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m+\u001B[39mseg_background_crop[seg_log_crop\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m1\u001B[39m])\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m2\u001B[39m\n\u001B[1;32m    307\u001B[0m seg_background_temp[seg_log_crop\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m=\u001B[39mseg_crop_output_np_bg[seg_log_crop\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m--> 309\u001B[0m seg_boundary_temp[seg_log_crop\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m=\u001B[39m(seg_crop_output_np_bd[\u001B[43mseg_log_crop\u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m]\u001B[38;5;241m+\u001B[39mseg_boundary_crop[seg_log_crop\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m1\u001B[39m])\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m2\u001B[39m\n\u001B[1;32m    310\u001B[0m seg_boundary_temp[seg_log_crop\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m=\u001B[39mseg_crop_output_np_bd[seg_log_crop\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    312\u001B[0m seg_foreground_temp[seg_log_crop\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m=\u001B[39m(seg_crop_output_np_fg[seg_log_crop\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m+\u001B[39mseg_foreground_crop[seg_log_crop\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m1\u001B[39m])\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m2\u001B[39m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "img_ws_predictions = {}\n",
    "\n",
    "for img_name in HMS_data_dict_train.keys():\n",
    "    print(img_name)\n",
    "    raw_img_path = HMS_data_dict_train[img_name][\"raw\"]\n",
    "    print(HMS_data_dict_train[img_name][\"raw\"])\n",
    "    hand_seg_path = f\"data/CellSeg_dataset/HMS_processed/segmentation/{img_name}/{img_name}_ins.npy\"\n",
    "\n",
    "    raw_img = np.load(raw_img_path)\n",
    "    hand_seg = np.load(hand_seg_path)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # feed the raw img to the model\n",
    "    print('Feed raw img to model')\n",
    "    raw_img_size=raw_img.shape\n",
    "\n",
    "    seg_background_comp = np.zeros(raw_img_size)\n",
    "    seg_boundary_comp = np.zeros(raw_img_size)\n",
    "\n",
    "    transposes = [[0,1,2]]#,[2,0,1],[0,2,1]]\n",
    "    reverse_transposes = [[0,1,2]]#,[1,2,0],[0,2,1]]\n",
    "\n",
    "    for idx, transpose in enumerate(transposes):\n",
    "        print(str(idx+1)+\": Transpose the image to be: \"+str(transpose))\n",
    "        with torch.no_grad():\n",
    "            seg_img=\\\n",
    "            semantic_segment_crop_and_cat_3_channel_output(raw_img.transpose(transpose), model, device, crop_cube_size=crop_cube_size, stride=stride)\n",
    "        seg_img_background=seg_img['background']\n",
    "        seg_img_boundary=seg_img['boundary']\n",
    "        seg_img_foreground=seg_img['foreground']\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # argmax\n",
    "        print('argmax', end='\\r')\n",
    "        seg=[]\n",
    "        seg.append(seg_img_background)\n",
    "        seg.append(seg_img_boundary)\n",
    "        seg.append(seg_img_foreground)\n",
    "        seg=np.array(seg)\n",
    "        seg_argmax=np.argmax(seg, axis=0)\n",
    "        # probability map to 0 1 segment\n",
    "        seg_background=np.zeros(seg_img_background.shape)\n",
    "        seg_background[np.where(seg_argmax==0)]=1\n",
    "        seg_foreground=np.zeros(seg_img_foreground.shape)\n",
    "        seg_foreground[np.where(seg_argmax==2)]=1\n",
    "        seg_boundary=np.zeros(seg_img_boundary.shape)\n",
    "        seg_boundary[np.where(seg_argmax==1)]=1\n",
    "\n",
    "        seg_background=seg_background.transpose(reverse_transposes[idx])\n",
    "        seg_foreground=seg_foreground.transpose(reverse_transposes[idx])\n",
    "        seg_boundary=seg_boundary.transpose(reverse_transposes[idx])\n",
    "\n",
    "        seg_background_comp+=seg_background\n",
    "        seg_boundary_comp+=seg_boundary\n",
    "    #print(\"Get model semantic seg by combination\")\n",
    "    seg_background_comp = np.array(seg_background_comp>0, dtype=float)\n",
    "    seg_boundary_comp = np.array(seg_boundary_comp>0, dtype=float)\n",
    "    seg_foreground_comp = np.array(1 - seg_background_comp - seg_boundary_comp>0, dtype=float)\n",
    "\n",
    "\n",
    "    how_close_are_the_super_vox_to_boundary=2\n",
    "    min_touching_percentage=0.51\n",
    "\n",
    "    seg_foreground_erosion=1-img_3d_erosion_or_expansion(1-seg_foreground_comp, kernel_size=how_close_are_the_super_vox_to_boundary+1, device=device)\n",
    "    seg_foreground_super_voxel_by_ws = generate_super_vox_by_watershed(seg_foreground_erosion, connectivity=min_touching_area)\n",
    "\n",
    "    img_ws_predictions[img_name] = seg_foreground_super_voxel_by_ws\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed: \", end - start)\n",
    "\n",
    "np.save(\"img_ws_predictions.npy\", img_ws_predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}