{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import edt\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from skimage.metrics import adapted_rand_error\n",
    "\n",
    "import torch\n",
    "from torch import from_numpy as from_numpy\n",
    "from torchsummary import summary\n",
    "\n",
    "from func.run_pipeline_super_vox import segment_super_vox_3_channel, semantic_segment_crop_and_cat_3_channel_output, img_3d_erosion_or_expansion, \\\n",
    "generate_super_vox_by_watershed, get_outlayer_of_a_3d_shape, get_crop_by_pixel_val, Cluster_Super_Vox, assign_boudary_voxels_to_cells_with_watershed, \\\n",
    "delete_too_small_cluster, reassign\n",
    "from func.run_pipeline import segment, assign_boudary_voxels_to_cells, dbscan_of_seg, semantic_segment_crop_and_cat\n",
    "from func.cal_accuracy import IOU_and_Dice_Accuracy, VOI\n",
    "from func.network import VoxResNet, CellSegNet_basic_lite\n",
    "from func.unet_3d_basic import UNet3D_basic\n",
    "from func.ultis import save_obj, load_obj\n",
    "\n",
    "model=CellSegNet_basic_lite(input_channel=1, n_classes=3, output_func = \"softmax\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "load_path='output/model_HMS_delete_fake_cells.pkl'\n",
    "checkpoint = torch.load(load_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "HMS_data_dict = load_obj(\"dataset_info/HMS_dataset_info\")\n",
    "HMS_data_dict_test = HMS_data_dict[\"test\"]\n",
    "\n",
    "# we do not input the whole raw image to the model one time but input raw image crops\n",
    "crop_cube_size=64\n",
    "stride=32\n",
    "\n",
    "# hyperparameter for TASCAN, min touching area of two super pixels if they belong to the same cell\n",
    "min_touching_area=30\n",
    "\n",
    "print(\"Test cases: \"+str(HMS_data_dict_test.keys()))\n",
    "case = \"135\"\n",
    "print(\"for test case \"+str(case)+\" : \"+str(HMS_data_dict_test[case]))\n",
    "\n",
    "# you may load the image using another path\n",
    "raw_img=np.load(HMS_data_dict_test[case][\"raw\"]) # .astype(float)\n",
    "print(HMS_data_dict_test[case][\"raw\"])\n",
    "hand_seg=np.load(HMS_data_dict_test[case][\"ins\"]).astype(float)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# feed the raw img to the model\n",
    "print('Feed raw img to model')\n",
    "raw_img_size=raw_img.shape\n",
    "\n",
    "seg_background_comp = np.zeros(raw_img_size)\n",
    "seg_boundary_comp = np.zeros(raw_img_size)\n",
    "\n",
    "transposes = [[0,1,2]]#,[2,0,1],[0,2,1]]\n",
    "reverse_transposes = [[0,1,2]]#,[1,2,0],[0,2,1]]\n",
    "\n",
    "for idx, transpose in enumerate(transposes):\n",
    "    print(str(idx+1)+\": Transpose the image to be: \"+str(transpose))\n",
    "    # with torch.no_grad():\n",
    "    seg_img=\\\n",
    "    semantic_segment_crop_and_cat_3_channel_output(raw_img.transpose(transpose), model, device, crop_cube_size=crop_cube_size, stride=stride)\n",
    "    seg_img_background=seg_img['background']\n",
    "    seg_img_boundary=seg_img['boundary']\n",
    "    seg_img_foreground=seg_img['foreground']\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # argmax\n",
    "    print('argmax', end='\\r')\n",
    "    seg=[]\n",
    "    seg.append(seg_img_background)\n",
    "    seg.append(seg_img_boundary)\n",
    "    seg.append(seg_img_foreground)\n",
    "    seg=np.array(seg)\n",
    "    seg_argmax=np.argmax(seg, axis=0)\n",
    "    # probability map to 0 1 segment\n",
    "    seg_background=np.zeros(seg_img_background.shape)\n",
    "    seg_background[np.where(seg_argmax==0)]=1\n",
    "    seg_foreground=np.zeros(seg_img_foreground.shape)\n",
    "    seg_foreground[np.where(seg_argmax==2)]=1\n",
    "    seg_boundary=np.zeros(seg_img_boundary.shape)\n",
    "    seg_boundary[np.where(seg_argmax==1)]=1\n",
    "\n",
    "    seg_background=seg_background.transpose(reverse_transposes[idx])\n",
    "    seg_foreground=seg_foreground.transpose(reverse_transposes[idx])\n",
    "    seg_boundary=seg_boundary.transpose(reverse_transposes[idx])\n",
    "\n",
    "    seg_background_comp+=seg_background\n",
    "    seg_boundary_comp+=seg_boundary\n",
    "#print(\"Get model semantic seg by combination\")\n",
    "seg_background_comp = np.array(seg_background_comp>0, dtype=float)\n",
    "seg_boundary_comp = np.array(seg_boundary_comp>0, dtype=float)\n",
    "seg_foreground_comp = np.array(1 - seg_background_comp - seg_boundary_comp>0, dtype=float)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time elapsed: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# np.save(raw_img, \"graph_construction_test/raw_img_test\")\n",
    "\n",
    "with open(\"graph_construction_test/raw_img_test.npy\", 'wb') as f:\n",
    "    np.save(f, raw_img)\n",
    "\n",
    "with open(\"graph_construction_test/hand_seg_test.npy\", 'wb') as f:\n",
    "    np.save(f, hand_seg)\n",
    "\n",
    "with open(\"graph_construction_test/seg_foreground_comp_test.npy\", 'wb') as f:\n",
    "    np.save(f, seg_foreground_comp)\n",
    "\n",
    "with open(\"graph_construction_test/seg_boundary_comp_test.npy\", 'wb') as f:\n",
    "    np.save(f, seg_boundary_comp)\n",
    "\n",
    "with open(\"graph_construction_test/seg_background_test.npy\", 'wb') as f:\n",
    "    np.save(f, seg_background)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "raw_img = np.load(\"graph_construction_test/raw_img_test.npy\")\n",
    "hand_seg = np.load(\"graph_construction_test/hand_seg_test.npy\")\n",
    "seg_foreground_comp = np.load(\"graph_construction_test/seg_foreground_comp_test.npy\")\n",
    "seg_boundary_comp = np.load(\"graph_construction_test/seg_boundary_comp_test.npy\")\n",
    "seg_background = np.load(\"graph_construction_test/seg_background_test.npy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}