{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "annoying-cement",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "following-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import edt\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from skimage.metrics import adapted_rand_error\n",
    "\n",
    "import torch\n",
    "from torch import from_numpy as from_numpy\n",
    "from torchsummary import summary\n",
    "\n",
    "from func.run_pipeline_super_vox import segment_super_vox_3_channel, semantic_segment_crop_and_cat_3_channel_output, \\\n",
    "img_3d_erosion_or_expansion, segment_super_vox_2_channel, semantic_segment_crop_and_cat_2_channel_output, \\\n",
    "generate_super_vox_by_watershed, get_outlayer_of_a_3d_shape, get_crop_by_pixel_val, Cluster_Super_Vox, assign_boudary_voxels_to_cells_with_watershed, \\\n",
    "delete_too_small_cluster, reassign\n",
    "from func.run_pipeline import segment, assign_boudary_voxels_to_cells, dbscan_of_seg, semantic_segment_crop_and_cat\n",
    "from func.cal_accuracy import IOU_and_Dice_Accuracy, VOI\n",
    "from func.network import VoxResNet, CellSegNet_basic_lite_w_groupnorm\n",
    "from func.unet_3d_basic import UNet3D_basic\n",
    "from func.ultis import save_obj, load_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-blowing",
   "metadata": {},
   "source": [
    "### init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lesser-classics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "CellSegNet_basic_lite_w_groupnorm(\n  (conv1): Conv3d(1, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n  (conv2): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  (bnorm1): GroupNorm(1, 32, eps=1e-05, affine=True)\n  (conv3): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n  (resmodule1): ResModule_w_groupnorm(\n    (groupnorm_module): GroupNorm(1, 64, eps=1e-05, affine=True)\n    (conv_module): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  )\n  (conv4): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n  (resmodule2): ResModule_w_groupnorm(\n    (groupnorm_module): GroupNorm(1, 64, eps=1e-05, affine=True)\n    (conv_module): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  )\n  (conv5): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n  (resmodule3): ResModule_w_groupnorm(\n    (groupnorm_module): GroupNorm(1, 64, eps=1e-05, affine=True)\n    (conv_module): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  )\n  (deconv1): ConvTranspose3d(64, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n  (bnorm2): GroupNorm(1, 64, eps=1e-05, affine=True)\n  (deconv2): ConvTranspose3d(64, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n  (bnorm3): GroupNorm(1, 64, eps=1e-05, affine=True)\n  (deconv3): ConvTranspose3d(64, 32, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n  (bnorm4): GroupNorm(1, 32, eps=1e-05, affine=True)\n  (conv6): Conv3d(32, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model=UNet3D_basic(in_channels = 1, out_channels = 3)\n",
    "# load_path=''\n",
    "# model=VoxResNet(input_channel=1, n_classes=3, output_func = \"softmax\")\n",
    "# load_path=''\n",
    "model=CellSegNet_basic_lite_w_groupnorm(input_channel=1, n_classes=3, output_func = \"softmax\")\n",
    "load_path='output/model_Ovules_retrained_groupnorm.pkl'\n",
    "checkpoint = torch.load(load_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf35945b",
   "metadata": {},
   "source": [
    "### dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3389468",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ovules_data_dict = load_obj(\"dataset_info/Ovules_dataset_info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feaf2e6",
   "metadata": {},
   "source": [
    "### seg one img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b17a9d",
   "metadata": {},
   "source": [
    "parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22684319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do not input the whole raw image to the model one time but input raw image crops\n",
    "crop_cube_size=128\n",
    "stride=64\n",
    "\n",
    "# hyperparameter for TASCAN, min touching area of two super pixels if they belong to the same cell\n",
    "min_touching_area=30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faff77f",
   "metadata": {},
   "source": [
    "choose a test image and load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "finnish-joint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are test imgs: dict_keys(['N_441_final_crop_ds2.npz', 'N_522_final_crop_ds2.npz', 'N_511_final_crop_ds2.npz', 'N_435_final_crop_ds2.npz', 'N_294_final_crop_ds2.npz', 'N_590_final_crop_ds2.npz', 'N_593_final_crop_ds2.npz'])\n",
      "for test case N_435_final_crop_ds2.npz : data/CellSeg_dataset/Ovules_processed_thin_boundary/test/N_435_final_crop_ds2.npz\n"
     ]
    }
   ],
   "source": [
    "print(\"there are test imgs: \"+str(Ovules_data_dict['test'].keys()))\n",
    "case = 'N_435_final_crop_ds2.npz'\n",
    "print(\"for test case \"+str(case)+\" : \"+str(Ovules_data_dict['test'][case]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "placed-intranet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qq/9q6wphrj19z8xt4qcffm8df80000gn/T/ipykernel_69379/2859134447.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  raw_img = np.array(hf[\"raw\"], dtype=np.float)\n",
      "/var/folders/qq/9q6wphrj19z8xt4qcffm8df80000gn/T/ipykernel_69379/2859134447.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  hand_seg = np.array(hf[\"ins\"], dtype=np.float)\n",
      "/var/folders/qq/9q6wphrj19z8xt4qcffm8df80000gn/T/ipykernel_69379/2859134447.py:4: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  boundary_gt = np.array(hf[\"boundary\"], dtype=np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_img shape: (194, 550, 555)\n",
      "hand_seg shape: (194, 550, 555)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qq/9q6wphrj19z8xt4qcffm8df80000gn/T/ipykernel_69379/2859134447.py:5: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  foreground_gt = np.array(hf[\"foreground\"], dtype=np.float)\n"
     ]
    }
   ],
   "source": [
    "hf = np.load(Ovules_data_dict['test'][case])\n",
    "raw_img = np.array(hf[\"raw\"], dtype=np.float)\n",
    "hand_seg = np.array(hf[\"ins\"], dtype=np.float)\n",
    "boundary_gt = np.array(hf[\"boundary\"], dtype=np.float)\n",
    "foreground_gt = np.array(hf[\"foreground\"], dtype=np.float)\n",
    "\n",
    "print(\"raw_img shape: \"+str(raw_img.shape))\n",
    "print(\"hand_seg shape: \"+str(hand_seg.shape))\n",
    "raw_img_shape = raw_img.shape\n",
    "hand_seg_shape = hand_seg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf6663e",
   "metadata": {},
   "source": [
    "feed raw image crops to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-simulation",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed raw img to model. Use different transposes\n",
      "1: Transpose the image to be: [0, 1, 2]\n",
      "Progress of segment_3d_img: 48%\r"
     ]
    }
   ],
   "source": [
    "# feed the raw img to the model\n",
    "print('Feed raw img to model. Use different transposes')\n",
    "raw_img_size=raw_img.shape\n",
    "    \n",
    "seg_foreground_comp = np.zeros(raw_img_size)\n",
    "seg_boundary_comp = np.zeros(raw_img_size)\n",
    "\n",
    "transposes = [[0,1,2]]#,[2,0,1],[0,2,1]]\n",
    "reverse_transposes = [[0,1,2]]#,[1,2,0],[0,2,1]]\n",
    "\n",
    "for idx, transpose in enumerate(transposes):\n",
    "    print(str(idx+1)+\": Transpose the image to be: \"+str(transpose))\n",
    "    with torch.no_grad():\n",
    "        seg_img=\\\n",
    "        semantic_segment_crop_and_cat_3_channel_output(raw_img.transpose(transpose), model, device, crop_cube_size=crop_cube_size, stride=stride)\n",
    "    seg_img_background=seg_img['background']\n",
    "    seg_img_boundary=seg_img['boundary']\n",
    "    seg_img_foreground=seg_img['foreground']\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # argmax\n",
    "    print('argmax', end='\\r')\n",
    "    seg=[]\n",
    "    seg.append(seg_img_background)\n",
    "    seg.append(seg_img_boundary)\n",
    "    seg.append(seg_img_foreground)\n",
    "    seg=np.array(seg)\n",
    "    seg_argmax=np.argmax(seg, axis=0)\n",
    "    # probability map to 0 1 segment\n",
    "    seg_background=np.zeros(seg_img_background.shape)\n",
    "    seg_background[np.where(seg_argmax==0)]=1\n",
    "    seg_foreground=np.zeros(seg_img_foreground.shape)\n",
    "    seg_foreground[np.where(seg_argmax==2)]=1\n",
    "    seg_boundary=np.zeros(seg_img_boundary.shape)\n",
    "    seg_boundary[np.where(seg_argmax==1)]=1\n",
    "        \n",
    "    seg_background=seg_background.transpose(reverse_transposes[idx])\n",
    "    seg_foreground=seg_foreground.transpose(reverse_transposes[idx])\n",
    "    seg_boundary=seg_boundary.transpose(reverse_transposes[idx])\n",
    "        \n",
    "    seg_foreground_comp+=seg_foreground\n",
    "    seg_boundary_comp+=seg_boundary\n",
    "print(\"Get model semantic seg by combination\")\n",
    "seg_boundary_comp = np.array(seg_boundary_comp>0, dtype=np.int)\n",
    "seg_foreground_comp = np.array(seg_foreground_comp>0, dtype=np.int)\n",
    "seg_foreground_comp[seg_boundary_comp>0]=0\n",
    "seg_background_comp = np.array(1 - seg_foreground_comp - seg_boundary_comp>0, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-lottery",
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "N=80\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"raw_img\")\n",
    "plt.imshow(raw_img[N,:,:])\n",
    "plt.figure()\n",
    "plt.title(\"hand_seg\")\n",
    "plt.imshow(reassign(hand_seg[N,:,:]))\n",
    "plt.figure()\n",
    "plt.title(\"model_seg_foreground\")\n",
    "plt.imshow(seg_foreground_comp[N,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeae7c5",
   "metadata": {},
   "source": [
    "TASCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0710c300",
   "metadata": {},
   "source": [
    "generate super vox by watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-place",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Generate super vox by watershed\n",
    "how_close_are_the_super_vox_to_boundary=2\n",
    "min_touching_percentage=0.51\n",
    "\n",
    "seg_foreground_erosion=1-img_3d_erosion_or_expansion(1-seg_foreground_comp, kernel_size=how_close_are_the_super_vox_to_boundary+1, device=device)\n",
    "seg_foreground_super_voxel_by_ws = generate_super_vox_by_watershed(seg_foreground_erosion, connectivity=min_touching_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-papua",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"There are \"+str(len(np.unique(seg_foreground_super_voxel_by_ws)))+\" super voxels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc60c87",
   "metadata": {},
   "source": [
    "super voxel clustearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-nirvana",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Super voxel clustearing\n",
    "cluster_super_vox=Cluster_Super_Vox(min_touching_area=min_touching_area, min_touching_percentage=min_touching_percentage)\n",
    "cluster_super_vox.fit(seg_foreground_super_voxel_by_ws)\n",
    "seg_foreground_single_cell_with_boundary = cluster_super_vox.output_3d_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b95740",
   "metadata": {},
   "source": [
    "delete too small cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-region",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Delete too small cells\n",
    "min_cell_size_threshold=10\n",
    "\n",
    "seg_foreground_single_cell_with_boundary_delete_too_small = delete_too_small_cluster(seg_foreground_single_cell_with_boundary, threshold=min_cell_size_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2bfc89",
   "metadata": {},
   "source": [
    "assign boudary voxels to their nearest cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-grounds",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Assign boudary voxels to their nearest cells\n",
    "seg_final=assign_boudary_voxels_to_cells_with_watershed(seg_foreground_single_cell_with_boundary_delete_too_small, seg_boundary_comp, seg_background_comp, compactness=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-warrior",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Reassign unique numbers\n",
    "# seg_final=reassign(seg_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a707d24a",
   "metadata": {},
   "source": [
    "see the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-princeton",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "N=80\n",
    "# print(\"There are \"+str(len(np.unique(seg_final)))+\" cells in model prediction\")\n",
    "# print(\"There are \"+str(len(np.unique(hand_seg)))+\" cells in hand seg\")\n",
    "plt.figure()\n",
    "plt.title(\"raw_img\")\n",
    "plt.imshow(raw_img[N,:,:])\n",
    "plt.figure()\n",
    "plt.title(\"hand_seg\")\n",
    "plt.imshow(reassign(hand_seg[N,:,:]))\n",
    "plt.figure()\n",
    "plt.title(\"model_seg\")\n",
    "plt.imshow(reassign(seg_final[N,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cded9c",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def colorful_seg(seg):\n",
    "    unique_vals, val_counts = np.unique(seg, return_counts=True)\n",
    "    \n",
    "    background_val = unique_vals[np.argsort(val_counts)[::-1][0]]\n",
    "    \n",
    "    seg_RGB = []\n",
    "    for i in range(seg.shape[0]):\n",
    "        mask_gray = cv2.normalize(src=seg[i,:,:], dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
    "        seg_slice_RGB = cv2.cvtColor(mask_gray, cv2.COLOR_GRAY2RGB)\n",
    "        seg_RGB.append(seg_slice_RGB)\n",
    "    seg_RGB = np.array(seg_RGB)\n",
    "    \n",
    "    for idx, unique_val in enumerate(unique_vals):\n",
    "        print(str(idx/len(unique_vals)), end=\"\\r\")\n",
    "        if unique_val == background_val:\n",
    "            COLOR = np.array([0,0,0], dtype=int)\n",
    "        else:\n",
    "            COLOR = np.array(np.random.choice(np.arange(256), size=3, replace=False), dtype=int)\n",
    "        \n",
    "        locs = np.where(seg==unique_val)\n",
    "        \n",
    "        for i in range(3):\n",
    "            seg_RGB[locs[0], locs[1], locs[2], i] = COLOR[i]\n",
    "        \n",
    "    return seg_RGB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b32a35",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "seg_final_RGB = colorful_seg(seg_final)\n",
    "hand_seg_RGB = colorful_seg(hand_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8972b6d7",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "N=80\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.axis('off')\n",
    "plt.imshow(seg_final_RGB[N,:,:])\n",
    "#plt.savefig('Ovules_seg_RGB_'+str(N)+'.png',bbox_inches='tight',dpi=fig.dpi,pad_inches=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ARE = adapted_rand_error(hand_seg.astype(int).flatten(), seg_final.astype(int).flatten())\n",
    "ARI = adjusted_rand_score(hand_seg.flatten(), seg_final.flatten())\n",
    "VOI_val = VOI(seg_final.astype(np.int),hand_seg.astype(np.int))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"ARI: \"+str(ARI))\n",
    "print(\"ARE: \"+str(ARE))\n",
    "print(\"VOI: \"+str(VOI_val))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def img_3d_interpolate(img_3d, output_size, device = torch.device('cpu'), mode='nearest'):\n",
    "    img_3d = img_3d.reshape(1,1,img_3d.shape[0],img_3d.shape[1],img_3d.shape[2])\n",
    "    img_3d=torch.from_numpy(img_3d).float().to(device)\n",
    "    img_3d=torch.nn.functional.interpolate(img_3d, size=output_size, mode='nearest')\n",
    "    img_3d=img_3d.detach().cpu().numpy()\n",
    "    img_3d=img_3d.reshape(img_3d.shape[2],img_3d.shape[3],img_3d.shape[4])\n",
    "\n",
    "    return img_3d\n",
    "\n",
    "scale_factor = 0.3\n",
    "org_shape = seg_final.shape\n",
    "output_size = (int(org_shape[0]*scale_factor), int(org_shape[1]*scale_factor), int(org_shape[2]*scale_factor))\n",
    "print(str(org_shape)+\" --> \"+str(output_size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy=IOU_and_Dice_Accuracy(img_3d_interpolate(hand_seg, output_size = output_size), img_3d_interpolate(seg_final, output_size = output_size))\n",
    "accuracy_record=accuracy.cal_accuracy_II()\n",
    "\n",
    "iou=np.array(accuracy_record[:,1]>0.7, dtype=np.float)\n",
    "print('cell count accuracy iou >0.7: '+str(sum(iou)/len(iou)))\n",
    "\n",
    "dice=np.array(accuracy_record[:,2]>0.7, dtype=np.float)\n",
    "print('cell count accuracy dice >0.7: '+str(sum(dice)/len(dice)))\n",
    "\n",
    "iou=np.array(accuracy_record[:,1]>0.5, dtype=np.float)\n",
    "print('cell count accuracy iou >0.5: '+str(sum(iou)/len(iou)))\n",
    "\n",
    "dice=np.array(accuracy_record[:,2]>0.5, dtype=np.float)\n",
    "print('cell count accuracy dice >0.5: '+str(sum(dice)/len(dice)))\n",
    "\n",
    "print('avg iou: '+str(np.mean(accuracy_record[:,1])))\n",
    "print('avg dice: '+str(np.mean(accuracy_record[:,2])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}