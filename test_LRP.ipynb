{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import edt\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from skimage.metrics import adapted_rand_error\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "from torch import from_numpy as from_numpy\n",
    "from torchsummary import summary\n",
    "\n",
    "from func.run_pipeline_super_vox import segment_super_vox_2_channel, semantic_segment_crop_and_cat_2_channel_output, img_3d_erosion_or_expansion, \\\n",
    "generate_super_vox_by_watershed, get_outlayer_of_a_3d_shape, get_crop_by_pixel_val, Cluster_Super_Vox, assign_boudary_voxels_to_cells_with_watershed, \\\n",
    "delete_too_small_cluster, reassign\n",
    "from func.run_pipeline import segment, assign_boudary_voxels_to_cells, dbscan_of_seg, semantic_segment_crop_and_cat\n",
    "from func.cal_accuracy import IOU_and_Dice_Accuracy, VOI\n",
    "from func.network import VoxResNet, CellSegNet_basic_lite\n",
    "from func.unet_3d_basic import UNet3D_basic\n",
    "from func.ultis import save_obj, load_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "CellSegNet_basic_lite(\n  (conv1): Conv3d(1, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n  (conv2): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  (bnorm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv3): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n  (resmodule1): ResModule(\n    (batchnorm_module): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv_module): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  )\n  (conv4): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n  (resmodule2): ResModule(\n    (batchnorm_module): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv_module): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  )\n  (conv5): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n  (resmodule3): ResModule(\n    (batchnorm_module): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv_module): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n  )\n  (deconv1): ConvTranspose3d(64, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n  (bnorm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (deconv2): ConvTranspose3d(64, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n  (bnorm3): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (deconv3): ConvTranspose3d(64, 32, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n  (bnorm4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv6): Conv3d(32, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model=UNet3D_basic(in_channels = 1, out_channels = 2)\n",
    "#load_path=''\n",
    "#model=VoxResNet(input_channel=1, n_classes=2, output_func = \"softmax\")\n",
    "#load_path=''\n",
    "model=CellSegNet_basic_lite(input_channel=1, n_classes=2, output_func = \"softmax\")\n",
    "load_path='output/model_LRP_retrained.pkl'\n",
    "checkpoint = torch.load(load_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "#summary(model, (1, 64, 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = load_obj(\"dataset_info/LRP_dataset_info\")\n",
    "data_dict_test = data_dict[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seg one img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do not input the whole raw image to the model one time but input raw image crops\n",
    "crop_cube_size=128\n",
    "stride=64\n",
    "\n",
    "# hyperparameter for TASCAN, min touching area of two super pixels if they belong to the same cell\n",
    "min_touching_area=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose a test image and load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are test imgs: dict_keys(['Movie2_T00010_crop_gt.h5', 'Movie1_t00006_crop_gt.h5', 'Movie1_t00045_crop_gt.h5', 'Movie2_T00020_crop_gt.h5'])\n",
      "for test case Movie2_T00010_crop_gt.h5 : data/CellSeg_dataset/LateralRootPrimordia_processed_wide_boundary/test/Movie2_T00010_crop_gt.h5\n"
     ]
    }
   ],
   "source": [
    "print(\"there are test imgs: \"+str(data_dict_test.keys()))\n",
    "case = 'Movie2_T00010_crop_gt.h5'\n",
    "print(\"for test case \"+str(case)+\" : \"+str(data_dict_test[case]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['background', 'boundary', 'edge_background', 'edge_boundary', 'edge_foreground', 'foreground', 'ins', 'raw']>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qq/9q6wphrj19z8xt4qcffm8df80000gn/T/ipykernel_7955/1358275588.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  raw_img = np.array(hf[\"raw\"], dtype=np.float)\n",
      "/var/folders/qq/9q6wphrj19z8xt4qcffm8df80000gn/T/ipykernel_7955/1358275588.py:4: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  hand_seg = np.array(hf[\"ins\"], dtype=np.float)\n",
      "/var/folders/qq/9q6wphrj19z8xt4qcffm8df80000gn/T/ipykernel_7955/1358275588.py:5: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  boundary_gt = np.array(hf[\"boundary\"], dtype=np.float)\n",
      "/var/folders/qq/9q6wphrj19z8xt4qcffm8df80000gn/T/ipykernel_7955/1358275588.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  background_gt = np.array(hf[\"background\"], dtype=np.float)\n",
      "/var/folders/qq/9q6wphrj19z8xt4qcffm8df80000gn/T/ipykernel_7955/1358275588.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  foreground_gt = np.array(hf[\"foreground\"], dtype=np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_img shape: (201, 225, 795)\n",
      "hand_seg shape: (201, 225, 795)\n"
     ]
    }
   ],
   "source": [
    "hf = h5py.File(data_dict_test[case], 'r')\n",
    "print(hf.keys())\n",
    "raw_img = np.array(hf[\"raw\"], dtype=np.float)\n",
    "hand_seg = np.array(hf[\"ins\"], dtype=np.float)\n",
    "boundary_gt = np.array(hf[\"boundary\"], dtype=np.float)\n",
    "background_gt = np.array(hf[\"background\"], dtype=np.float)\n",
    "foreground_gt = np.array(hf[\"foreground\"], dtype=np.float)\n",
    "\n",
    "print(\"raw_img shape: \"+str(raw_img.shape))\n",
    "print(\"hand_seg shape: \"+str(hand_seg.shape))\n",
    "raw_img_shape = raw_img.shape\n",
    "hand_seg_shape = hand_seg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feed raw image crops to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed raw img to model. Use different transposes\n",
      "1: Transpose the image to be: [0, 1, 2]\n",
      "argmax\rs of segment_3d_img: 99%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qq/9q6wphrj19z8xt4qcffm8df80000gn/T/ipykernel_7955/4072038333.py:24: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  seg_foreground=np.array(seg_img_foreground-seg_img_boundary>0, dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: Transpose the image to be: [2, 0, 1]\n",
      "argmax\rs of segment_3d_img: 99%3: Transpose the image to be: [0, 2, 1]\n",
      "Progress of segment_3d_img: 81%\r"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "transposes = [[0,1,2],[2,0,1],[0,2,1]]#,[1,0,2]]\n",
    "reverse_transposes = [[0,1,2],[1,2,0],[0,2,1]]#,[1,0,2]]\n",
    "\n",
    "# feed the raw img to the model\n",
    "print('Feed raw img to model. Use different transposes')\n",
    "raw_img_size=raw_img.shape\n",
    "seg_boundary_comp = np.zeros(raw_img_size)\n",
    "seg_img_boundary_comp = np.zeros(raw_img_size)\n",
    "for idx, transpose in enumerate(transposes):\n",
    "    print(str(idx+1)+\": Transpose the image to be: \"+str(transpose))\n",
    "    with torch.no_grad():\n",
    "        seg_img=\\\n",
    "        semantic_segment_crop_and_cat_2_channel_output(raw_img.transpose(transpose), model, device,\n",
    "                                                       crop_cube_size=crop_cube_size, stride=stride)\n",
    "    seg_img_boundary=seg_img['boundary']\n",
    "    seg_img_foreground=seg_img['foreground']\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # argmax\n",
    "    print('argmax', end='\\r')\n",
    "    # probability map to 0 1 segment\n",
    "    seg_foreground=np.array(seg_img_foreground-seg_img_boundary>0, dtype=np.int)\n",
    "    seg_boundary=1 - seg_foreground\n",
    "        \n",
    "    seg_foreground=seg_foreground.transpose(reverse_transposes[idx])\n",
    "    seg_boundary=seg_boundary.transpose(reverse_transposes[idx])\n",
    "    seg_img_foreground=seg_img_foreground.transpose(reverse_transposes[idx])\n",
    "    seg_img_boundary=seg_img_boundary.transpose(reverse_transposes[idx])\n",
    "    \n",
    "    seg_boundary_comp+=seg_boundary\n",
    "    seg_img_boundary_comp+=seg_img_boundary\n",
    "\n",
    "print(\"Get model semantic seg by combination\")\n",
    "seg_boundary_comp = np.array(seg_boundary_comp>0, dtype=np.int)\n",
    "seg_foreground_comp = 1 - seg_boundary_comp\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time elapsed: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# show current result\n",
    "\n",
    "N=200\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"raw_img\")\n",
    "plt.imshow(raw_img[:,:,N])\n",
    "plt.figure()\n",
    "plt.title(\"hand_seg\")\n",
    "plt.imshow(reassign(hand_seg[:,:,N]))\n",
    "plt.figure()\n",
    "plt.title(\"model_seg_boundary\")\n",
    "plt.imshow(seg_boundary_comp[:,:,N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate super vox by watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Generate super vox by watershed\n",
    "how_close_are_the_super_vox_to_boundary=2\n",
    "min_touching_percentage=0.51\n",
    "\n",
    "seg_foreground_erosion=1-img_3d_erosion_or_expansion(1-seg_foreground_comp, kernel_size=how_close_are_the_super_vox_to_boundary+1, device=device)\n",
    "seg_foreground_super_voxel_by_ws = generate_super_vox_by_watershed(seg_foreground_erosion)\n",
    "# from skimage.measure import label\n",
    "# from skimage.segmentation import join_segmentations, watershed\n",
    "# from skimage.feature import peak_local_max\n",
    "# seg_foreground_edt=edt.edt(np.array(seg_foreground_erosion, dtype=np.uint32, order='F'),\n",
    "#                            black_border=True, order='F',parallel=1)\n",
    "# min_distance_between_cells = 5\n",
    "# coords = peak_local_max(seg_foreground_edt, min_distance=min_distance_between_cells,\n",
    "#                         labels=np.array(seg_foreground_erosion>0))\n",
    "# mask = np.zeros(seg_foreground_edt.shape, dtype=bool)\n",
    "# mask[tuple(coords.T)] = True\n",
    "# markers = label(mask==True)\n",
    "# seg_foreground_super_voxel_by_ws = watershed(-seg_foreground_edt, markers=markers, mask=np.array(seg_foreground_comp>0), connectivity=min_touching_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"There are \"+str(len(np.unique(seg_foreground_super_voxel_by_ws)))+\" super voxels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "super voxel clustearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Super voxel clustering\n",
    "cluster_super_vox=Cluster_Super_Vox(min_touching_area=min_touching_area, min_touching_percentage=min_touching_percentage)\n",
    "cluster_super_vox.fit(copy.deepcopy(seg_foreground_super_voxel_by_ws))\n",
    "seg_foreground_single_cell_with_boundary = cluster_super_vox.output_3d_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete too small cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Delete too small cells\n",
    "min_cell_size_threshold=10\n",
    "seg_foreground_single_cell_with_boundary = delete_too_small_cluster(seg_foreground_single_cell_with_boundary, threshold=min_cell_size_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assign boudary voxels to their nearest cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Assign boudary voxels to their nearest cells\n",
    "seg_final=assign_boudary_voxels_to_cells_with_watershed(seg_foreground_single_cell_with_boundary, seg_boundary_comp, compactness=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "N=250\n",
    "#print(\"There are \"+str(len(np.unique(seg_foreground_single_cell_with_boundary)))+\" cells\")\n",
    "plt.figure()\n",
    "plt.title(\"raw_img\")\n",
    "plt.imshow(raw_img[:,:,N])\n",
    "plt.figure()\n",
    "plt.title(\"hand_seg\")\n",
    "plt.imshow(reassign(hand_seg[:,:,N]))\n",
    "plt.figure()\n",
    "plt.title(\"model_seg\")\n",
    "plt.imshow(reassign(seg_final[:,:,N]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def colorful_seg(seg):\n",
    "    unique_vals, val_counts = np.unique(seg, return_counts=True)\n",
    "    \n",
    "    background_val = unique_vals[np.argsort(val_counts)[::-1][0]]\n",
    "    \n",
    "    seg_RGB = []\n",
    "    for i in range(seg.shape[0]):\n",
    "        mask_gray = cv2.normalize(src=seg[i,:,:], dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
    "        seg_slice_RGB = cv2.cvtColor(mask_gray, cv2.COLOR_GRAY2RGB)\n",
    "        seg_RGB.append(seg_slice_RGB)\n",
    "    seg_RGB = np.array(seg_RGB)\n",
    "    \n",
    "    for idx, unique_val in enumerate(unique_vals):\n",
    "        print(str(idx/len(unique_vals)), end=\"\\r\")\n",
    "        if unique_val == background_val:\n",
    "            COLOR = np.array([0,0,0], dtype=int)\n",
    "        else:\n",
    "            COLOR = np.array(np.random.choice(np.arange(256), size=3, replace=False), dtype=int)\n",
    "        \n",
    "        locs = np.where(seg==unique_val)\n",
    "        \n",
    "        for i in range(3):\n",
    "            seg_RGB[locs[0], locs[1], locs[2], i] = COLOR[i]\n",
    "        \n",
    "    return seg_RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "seg_final_RGB = colorful_seg(seg_final)\n",
    "hand_seg_RGB = colorful_seg(hand_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "N=250\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.axis('off')\n",
    "plt.imshow(seg_final_RGB[:,:,N],cmap=\"gray\")\n",
    "#plt.savefig('seg_final_RGB_'+str(N)+'.png',bbox_inches='tight',dpi=fig.dpi,pad_inches=0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}